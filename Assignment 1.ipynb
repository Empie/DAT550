{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0163b57",
   "metadata": {},
   "source": [
    "Assignment 1 - Building a Random Forest\n",
    "This is a skeleton of a Random Forest classifier.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b9c04941",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "from statistics import median, mode, mean\n",
    "from collections import Counter\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statistics\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f599844",
   "metadata": {},
   "source": [
    "Some simple type definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41d87eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttrType(Enum):\n",
    "    cat = 0  # categorical (qualitative) attribute\n",
    "    num = 1  # numerical (quantitative) attribute\n",
    "    target = 2  # target label\n",
    "\n",
    "class NodeType(Enum):\n",
    "    root = 0\n",
    "    internal = 1\n",
    "    leaf = 2\n",
    "\n",
    "class SplitType(Enum):\n",
    "    bin = 0  # binary split\n",
    "    multi = 1  # multi-way split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6b985b",
   "metadata": {},
   "source": [
    "Also, some basic classes to represent an attribute, a spltting procedure, and a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ff84975",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attribute(object):\n",
    "    def __init__(self, label, type):\n",
    "        assert type in AttrType\n",
    "        self.label = label\n",
    "        self.type = type\n",
    "        self.stat = None  # holds mean for numerical and mode for categorical attributes\n",
    "\n",
    "\n",
    "class Splitting(object):\n",
    "    def __init__(self, attr, infogain, split_type, cond, splits):\n",
    "        self.attr = attr  # attribute ID (index in ATTR)\n",
    "        self.infogain = infogain  # information gain if splitting is done on this attribute\n",
    "        self.split_type = split_type  # one of SplitType\n",
    "        self.cond = cond  # splitting condition, i.e., values on outgoing edges\n",
    "        self.splits = splits  # list of training records (IDs) for each slitting condition\n",
    "\n",
    "\n",
    "class Node(object):\n",
    "    def __init__(self, id, type, parent_id, children=None, edge_value=None, val=None, split_type=None, split_cond=None,\n",
    "                 infogain=None):\n",
    "        self.id = id  # ID (same as the index in DT.model list)\n",
    "        self.type = type  # one of NodeType\n",
    "        self.parent_id = parent_id  # ID of parent node (None if root)\n",
    "        self.children = children  # list of IDs of child nodes\n",
    "        self.edge_value = edge_value  # the value of the incoming edge (only if not root node)\n",
    "        self.val = val  # if root or internal node: the attribute that is compared at that node; if leaf node: the target value\n",
    "        self.split_type = split_type  # one of SplitType\n",
    "        self.split_cond = split_cond  # splitting condition (median value for binary splits on numerical values; otherwise a list of categorical values (corresponding to child nodes))\n",
    "        self.infogain = infogain\n",
    "\n",
    "    def append_child(self, node_id):\n",
    "        self.children.append(node_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd5166b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('housing_price_train.csv')\n",
    "df['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].median())\n",
    "df.set_index('Id')\n",
    "df.to_csv('housing_price_train1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b111ac50",
   "metadata": {},
   "outputs": [],
   "source": [
    "ATTR = []\n",
    "INFILE = \"housing_price_train1.csv\"\n",
    "\n",
    "for col in df.columns:\n",
    "    if col == \"SalePrice\":\n",
    "        ATTR.append(Attribute(col, AttrType.target))\n",
    "    elif col == \"MSSubClass\" or col == \"OverallQual\" or col == \"OverallCond\":\n",
    "        ATTR.append(Attribute(col, AttrType.cat))\n",
    "    elif df[col].unique().dtype == \"int64\" or df[col].unique().dtype == \"float64\":\n",
    "        ATTR.append(Attribute(col, AttrType.num))        \n",
    "    else:\n",
    "        ATTR.append(Attribute(col, AttrType.cat))\n",
    "\n",
    "IDX_TARGET = len(ATTR) - 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d4b0a0",
   "metadata": {},
   "source": [
    "A class DT representing the decision tree classifier. It could represent with methods:\n",
    "\n",
    "a given impurity measure;\n",
    "the search for the best attribute to split with;\n",
    "the addition of a node to the tree;\n",
    "a convenient model printer;\n",
    "the recursive call for obtaining a tree;\n",
    "a builder and an applier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a5d23f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DT(object):\n",
    "    def __init__(self):\n",
    "        self.data = None  # training data set (loaded into memory)\n",
    "        self.model = None  # decision tree model\n",
    "        # self.default_target = 0.0  # default target class HVA ER DET FOR NOE?\n",
    "\n",
    "    def __load_data(self):\n",
    "        # with open(INFILE) as csvfile:\n",
    "        #     self.data = []\n",
    "        #     csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        #     next(csvreader)\n",
    "        #     for row in csvreader:\n",
    "        #         rec = []\n",
    "        #         for i in range(len(ATTR)):\n",
    "        #             val = row[i].strip()\n",
    "        #             # convert numerical attributes\n",
    "        #             if ATTR[i].type == AttrType.num:  # Note that this will break for \"?\" (missing attribute)\n",
    "        #                 if val == \"\":\n",
    "        #                     val = None\n",
    "        #                 else:\n",
    "        #                     val = float(val)\n",
    "        #             rec.append(val)\n",
    "        #         self.data.append(rec)\n",
    "        #     # print(self.data)\n",
    "        #         # self.data.append([element.strip() for element in row])  # strip spaces\n",
    "        self.data = df.values.tolist()\n",
    "                \n",
    "    def __median(self, a):\n",
    "        # numList = []\n",
    "        # for x in range(a):\n",
    "        #     numList.append(x)\n",
    "        # if len(numList) > 0:\n",
    "        #     return statistics.median(numList)\n",
    "        # else:\n",
    "        #     return 0\n",
    "        all_attribute_values = []\n",
    "        for idx in range(len(self.data)):\n",
    "            all_attribute_values.append(self.data[idx][a])\n",
    "        return statistics.median(all_attribute_values)\n",
    "\n",
    "    # HVA BRUKES DETTE TIL?\n",
    "    # def __mean_squared_error(self, records):\n",
    "    #     \"\"\"\n",
    "    #     Calculates mean squared error for a selection of records.\n",
    "\n",
    "    #     :param records: Data records (given by indices)\n",
    "    #     \"\"\"\n",
    "    #     # TODO\n",
    "    #     #print(records)\n",
    "    #     return 0\n",
    "        \n",
    "    def __find_best_attr(self, attrs, records):\n",
    "        \"\"\"\n",
    "        Finds the attribute with the largest gain.\n",
    "\n",
    "        :param attrs: Set of attributes\n",
    "        :param records: Training set (list of record ids)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # mse_p = self.__mean_squared_error(records)  # parent's MSE HVORFOR TRENGER VI DEN?\n",
    "        splittings = []  # holds the splitting information for each attribute\n",
    "\n",
    "        for a in attrs:\n",
    "            assert ATTR[a].type in AttrType\n",
    "            splits = {}  # record IDs corresponding to each split\n",
    "            # splitting condition depends on the attribute type\n",
    "            if ATTR[a].type == AttrType.target:  # skip target attribute\n",
    "                continue\n",
    "            elif ATTR[a].type == AttrType.cat:  # categorical attribute\n",
    "                # multi-way split on each possible value\n",
    "                split_mode = SplitType.multi\n",
    "                # each possible attr value corresponds to a split (indexed with categorical labels)\n",
    "                # Note: it's important to consider attr values from the entire training set\n",
    "                split_cond = set([self.data[idx][a] for idx in range(len(self.data))])\n",
    "\n",
    "                # TODO collect training records for each split \n",
    "                # `splits[val]` holds a list of records for a given split,\n",
    "                # where `val` is an element of `split_cond`\n",
    "                for val in split_cond:\n",
    "                    splits[val] = records\n",
    "                \n",
    "            elif ATTR[a].type == AttrType.num:  # numerical attribute => binary split on median value\n",
    "                split_mode = SplitType.bin\n",
    "                split_cond = self.__median(a)  # (i.e., if less or equal than this value)\n",
    "                \n",
    "                # TODO collect training records for each split (in `splits`)\n",
    "                splits[split_cond] = records\n",
    "            \n",
    "            # TODO compute gain for attribute a\n",
    "            infogain = 0\n",
    "            print(\"infogain\")\n",
    "            datalist = []\n",
    "            target = []\n",
    "            for idx in range(len(self.data)):\n",
    "                datalist.append(self.data[idx][a])\n",
    "                target.append(self.data[idx][-1])\n",
    "            target_df = pd.DataFrame(target, columns=[\"SalePrice\"])\n",
    "            datalist_df = pd.DataFrame(datalist)\n",
    "            #data_df = pd.DataFrame(self.data)\n",
    "            \n",
    "            data_split = self.__split(df, datalist_df[0])\n",
    "            split_labels = [dataframe['SalePrice'] for dataframe in data_split]\n",
    "            infogain = self.__calculate_information_gain(target_df, split_labels)\n",
    "            \n",
    "            print(infogain)\n",
    "            # p = len(y_true) / len(y)\n",
    "            # entropy = calculate_entropy(y)\n",
    "            # info_gain = entropy - p*calculate_entropy(y_true) - (1-p)*calculate_entropy(y_false)\n",
    "            splitting = Splitting(a, infogain, split_mode, split_cond, splits)\n",
    "            splittings.append(splitting)\n",
    "\n",
    "        # find best splitting\n",
    "        best_splitting = sorted(splittings, key=lambda x: x.infogain, reverse=True)[0]\n",
    "        return best_splitting\n",
    "    \n",
    "#########################################################################################\n",
    "########################## Funksjoner til å kalkulere infogain ##########################    \n",
    "#########################################################################################    \n",
    "    def __calculate_entropy(self, labels):\n",
    "        n_labels = len(labels)\n",
    "        if n_labels <= 1:\n",
    "            return 0\n",
    "        value, counts = np.unique(labels, return_counts=True)\n",
    "        probs = counts/n_labels\n",
    "        n_classes = len(value)\n",
    "        if n_classes <= 1:\n",
    "            return 0\n",
    "        entropy = 0\n",
    "        for i in probs:\n",
    "            entropy -= i*log(i,2)\n",
    "        return entropy\n",
    "    def __calculate_information_gain(self, target, split):\n",
    "        info_gain = self.__calculate_entropy(target)\n",
    "        for branch in split:\n",
    "            p = len(branch)/len(target)\n",
    "            info_gain -= p*self.__calculate_entropy(branch)\n",
    "        return info_gain\n",
    "    \n",
    "    def __split(self, dataset, datalist):\n",
    "        split_data = []\n",
    "        col_vals = datalist.unique()\n",
    "        for col_val in col_vals:\n",
    "            split_data.append(dataset[datalist == col_val])\n",
    "        return(split_data)\n",
    "#########################################################################################\n",
    "#########################################################################################\n",
    "\n",
    "    def __add_node(self, parent_id, node_type=NodeType.internal, edge_value=None, val=None, split_type=None,\n",
    "                   split_cond=None):\n",
    "        \"\"\"\n",
    "        Adds a node to the decision tree.\n",
    "\n",
    "        :param parent_id:\n",
    "        :param node_type:\n",
    "        :param edge_value:\n",
    "        :param val:\n",
    "        :param split_type:\n",
    "        :param split_cond:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        node_id = len(self.model)  # id of the newly assigned node\n",
    "        if not self.model:  # the tree is empty\n",
    "            node_type = NodeType.root\n",
    "\n",
    "        node = Node(node_id, node_type, parent_id, children=[], edge_value=edge_value, val=val, split_type=split_type,\n",
    "                    split_cond=split_cond)\n",
    "        self.model.append(node)\n",
    "\n",
    "        # also add it as a child of the parent node\n",
    "        if parent_id is not None:\n",
    "            self.model[parent_id].append_child(node_id)\n",
    "\n",
    "        return node_id\n",
    "\n",
    "    def __id3(self, attrs, records, parent_id=None, value=None):\n",
    "        \"\"\"\n",
    "        Function ID3 that returns a decision tree.\n",
    "\n",
    "        :param attrs: Set of attributes\n",
    "        :param records: Training set (list of record ids)\n",
    "        :param parent_id: ID of parent node\n",
    "        :param value: Value corresponding to the parent attribute, i.e., label of the edge on which we arrived to this node\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # empty training set or empty set of attributes => create leaf node with default class\n",
    "        if not records or not attrs:\n",
    "            self.__add_node(parent_id, node_type=NodeType.leaf, edge_value=value, val=self.default_class)\n",
    "            return\n",
    "\n",
    "        # if all records have the same target value => create leaf node with that target value\n",
    "        same = all(self.data[idx][IDX_TARGET] == self.data[records[0]][IDX_TARGET] for idx in records)\n",
    "        if same:\n",
    "            target = self.data[records[0]][IDX_TARGET]\n",
    "            self.__add_node(parent_id, node_type=NodeType.leaf, edge_value=value, val=target)\n",
    "            return\n",
    "\n",
    "        # find the attribute with the largest gain\n",
    "        splitting = self.__find_best_attr(attrs, records)\n",
    "        # add node\n",
    "        node_id = self.__add_node(parent_id, edge_value=value, val=splitting.attr, split_type=splitting.split_type,\n",
    "                                  split_cond=splitting.cond)\n",
    "        # TODO call tree construction recursively for each split\n",
    "\n",
    "    def print_model(self, node_id=0, level=0):\n",
    "        node = self.model[node_id]\n",
    "        indent = \"  \" * level\n",
    "        if node.type == NodeType.leaf:\n",
    "            print(indent + str(node.edge_value) + \" [Leaf node] class=\" + node.val)\n",
    "        else:\n",
    "            cond = \" <= \" + str(node.split_cond) if ATTR[node.val].type == AttrType.num else \" == ? \"\n",
    "            if node.type == NodeType.root:\n",
    "                print(\"[Root node] '\" + ATTR[node.val].label + \"'\" + cond)\n",
    "            else:\n",
    "                print(indent + str(node.edge_value) + \" [Internal node] '\" + ATTR[node.val].label + \"'\" + cond)\n",
    "            # print tree for child notes recursively\n",
    "            for n_id in node.children:\n",
    "                self.print_model(n_id, level + 1)\n",
    "\n",
    "    def build_model(self):\n",
    "        self.__load_data()\n",
    "        self.model = []  # holds the decision tree model, represented as a list of nodes\n",
    "        # Get majority class\n",
    "        #   Note: Counter returns a dictionary, most_common(x) returns a list with the x most common elements as\n",
    "        #         (key, count) tuples; we need to take the first element of the list and the first element of the tuple\n",
    "        # self.default_target = mean([x[IDX_TARGET] for x in self.data])\n",
    "        target_list = []\n",
    "        for x in self.data:\n",
    "            target_list.append(x[IDX_TARGET])\n",
    "        # self.default_target = [int(n) for n in target_list] TRENGER VI DEN??\n",
    "        self.__id3(set(range(len(ATTR) - 1)), list(range(len(self.data))))\n",
    "\n",
    "    def apply_model(self, record):\n",
    "        node = self.model[0]\n",
    "        while node.type != NodeType.leaf:\n",
    "        # TODO based on the value of the record's attribute that is tested in `node`,# set `node` to one of its child nodes until a leaf node is reached\n",
    "         return node.val\n",
    "         \n",
    "    def predict(self, records):\n",
    "        predictions = []\n",
    "        for record in records:\n",
    "            pred_val = self.apply_model(self, record)\n",
    "            # TODO append pred_val to predictions\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dcf82f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infogain\n",
      "8.84781349728874\n",
      "infogain\n",
      "1.849263983633801\n",
      "infogain\n",
      "0.610442656412366\n",
      "infogain\n",
      "3.963733911616462\n",
      "infogain\n",
      "8.045796138342425\n",
      "infogain\n",
      "0.03089926970536865\n",
      "infogain\n",
      "8.516924761551813\n",
      "infogain\n",
      "0.6440215958884274\n",
      "infogain\n",
      "0.4003719562550772\n",
      "infogain\n",
      "0.005516314368811948\n",
      "infogain\n",
      "0.6816741945463414\n",
      "infogain\n",
      "0.2153426562652248\n",
      "infogain\n",
      "2.8909992865094982\n",
      "infogain\n",
      "0.5492182772604884\n",
      "infogain\n",
      "0.07970893717415382\n",
      "infogain\n",
      "0.5480655391840662\n",
      "infogain\n",
      "1.040500600677247\n",
      "infogain\n",
      "1.6965937412508612\n",
      "infogain\n",
      "1.0707428915826407\n",
      "infogain\n",
      "4.693292332369614\n",
      "infogain\n",
      "3.830712178508223\n",
      "infogain\n",
      "0.5167591721566724\n",
      "infogain\n",
      "0.11854637368104051\n",
      "infogain\n",
      "1.611405344864275\n",
      "infogain\n",
      "1.6916198988760325\n",
      "infogain\n",
      "0.8219718806434338\n",
      "infogain\n",
      "3.4383069738898833\n",
      "infogain\n",
      "0.8687124178349854\n",
      "infogain\n",
      "0.34237692714002604\n",
      "infogain\n",
      "0.9191370416613404\n",
      "infogain\n",
      "1.207088697847347\n",
      "infogain\n",
      "0.5601423531453201\n",
      "infogain\n",
      "1.0956213246336428\n",
      "infogain\n",
      "1.516844678703995\n",
      "infogain\n",
      "5.670441276349091\n",
      "infogain\n",
      "0.7100309399168925\n",
      "infogain\n",
      "0.9949224628706614\n",
      "infogain\n",
      "7.38437065031463\n",
      "infogain\n",
      "7.331008189000008\n",
      "infogain\n",
      "0.13710479607792048\n",
      "infogain\n",
      "0.8707648626787473\n",
      "infogain\n",
      "0.2279092984219469\n",
      "infogain\n",
      "0.3175148515256735\n",
      "infogain\n",
      "7.543209542247777\n",
      "infogain\n",
      "3.6720995806663135\n",
      "infogain\n",
      "0.16349471470225746\n",
      "infogain\n",
      "7.769850168158179\n",
      "infogain\n",
      "0.5119724913396031\n",
      "infogain\n",
      "0.17843016561173086\n",
      "infogain\n",
      "0.7934135994285872\n",
      "infogain\n",
      "0.5473892920601899\n",
      "infogain\n",
      "0.9887589218965007\n",
      "infogain\n",
      "0.17172054318486613\n",
      "infogain\n",
      "0.9758789261959074\n",
      "infogain\n",
      "1.6823265393894102\n",
      "infogain\n",
      "0.3142222126145991\n",
      "infogain\n",
      "0.7844303809899739\n",
      "infogain\n",
      "4.910185458777291\n",
      "infogain\n",
      "1.3079802583599776\n",
      "infogain\n",
      "4.832640454572293\n",
      "infogain\n",
      "1.421439476589601\n",
      "infogain\n",
      "1.0900461250030384\n",
      "infogain\n",
      "6.150569680759001\n",
      "infogain\n",
      "0.752669191714473\n",
      "infogain\n",
      "0.7115917672477703\n",
      "infogain\n",
      "0.2848407322839387\n",
      "infogain\n",
      "3.473613923051745\n",
      "infogain\n",
      "3.800125709364879\n",
      "infogain\n",
      "1.13451828103332\n",
      "infogain\n",
      "0.1407854947218931\n",
      "infogain\n",
      "0.6627229873146666\n",
      "infogain\n",
      "0.04277363585361371\n",
      "infogain\n",
      "8.841816998999589\n",
      "infogain\n",
      "7.65390255866349\n",
      "infogain\n",
      "8.660744468273915\n",
      "infogain\n",
      "0.25915932751080933\n",
      "infogain\n",
      "2.0527332029979615\n",
      "infogain\n",
      "1.222006626115114\n",
      "infogain\n",
      "0.5821581227343463\n",
      "infogain\n",
      "0.6725478985160653\n"
     ]
    }
   ],
   "source": [
    "dt = DT()\n",
    "dt.build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c3af5e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "   \n",
    "A class RF representing the random forest classifier. It could represent with methods:\n",
    "\n",
    "subsampling from the dataset\n",
    "build a model with multiple decision trees\n",
    "apply the model\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10989065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RF(object):\n",
    "    def __init__(self):\n",
    "        self.data = None  # training data set (loaded into memory)\n",
    "        self.trees = None  # decision trees\n",
    "    \n",
    "    def __load_data(self):\n",
    "        pass\n",
    "        \n",
    "    def __subsampling(self, train_set, sample_size_ratio):\n",
    "        sample_number = round(len(self.data) * sample_size_ratio)\n",
    "        # TODO: generate a subsample with replacement\n",
    "    def build_model(self, train_set, split_conditions, sample_size_ratio, number_of_trees):\n",
    "        for i in range(number_of_trees):\n",
    "            sample = self.__subsampling(train_set, sample_size_ratio)\n",
    "            tree = DT() # build a tree with sample data and split conditions\n",
    "            self.trees.append(tree)\n",
    "\n",
    "    def predict(self, test_set):\n",
    "        rf_predictions = []\n",
    "        for row in test_set:\n",
    "            predictions = [tree.predict(row) for tree in self.trees]\n",
    "            rf_predictions.append(np.mean(predictions))\n",
    "        return rf_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94e5fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dt = DT()\n",
    "    dt.build_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
