{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1d) Check whether a tweet is attention-worthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Emil\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import spacy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # Ignore warnings\n",
    "nltk.download('stopwords') # Load Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # Load Lemmatizer\n",
    "cachedStopWords = list(set(stopwords.words(\"english\")))\n",
    "\n",
    "# Twitter Datasets\n",
    "df_worthy_train = pd.read_csv(\"data/1d/CT22_english_1D_attentionworthy_train.tsv\", sep='\\t')\n",
    "df_worthy_valid = pd.read_csv(\"data/1d/CT22_english_1D_attentionworthy_dev.tsv\", sep='\\t')\n",
    "df_worthy_test = pd.read_csv(\"data/1d/CT22_english_1D_attentionworthy_dev_test.tsv\", sep='\\t')\n",
    "\n",
    "# Top 3000 most commonly used english words + covid related medical terms (used as vocabulary for the vectorization process)\n",
    "topwords = pd.read_csv('words.txt', sep=\" \", header=None).values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (909, 5)\n",
      "Train shape: (3321, 5)\n",
      "Validation shape: (306, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>1.359351e+18</td>\n",
       "      <td>http://twitter.com/user/status/135935094335617...</td>\n",
       "      <td>India's gift of 100,000 COVID-19 vaccines arri...</td>\n",
       "      <td>no_not_interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>1.350166e+18</td>\n",
       "      <td>http://twitter.com/user/status/135016568806166...</td>\n",
       "      <td>Here’s what I’m doing while I wait my turn for...</td>\n",
       "      <td>no_not_interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>1.369750e+18</td>\n",
       "      <td>http://twitter.com/user/status/136974953915491...</td>\n",
       "      <td>This afternoon, I’m hosting an event with the ...</td>\n",
       "      <td>no_not_interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>1.350165e+18</td>\n",
       "      <td>http://twitter.com/user/status/135016499568693...</td>\n",
       "      <td>Help shops like mine stay open. Mask up, avoid...</td>\n",
       "      <td>no_not_interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>1.370008e+18</td>\n",
       "      <td>http://twitter.com/user/status/137000807648978...</td>\n",
       "      <td>As part of the ongoing nationwide vaccination ...</td>\n",
       "      <td>no_not_interesting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic      tweet_id                                          tweet_url  \\\n",
       "0  COVID-19  1.359351e+18  http://twitter.com/user/status/135935094335617...   \n",
       "1  COVID-19  1.350166e+18  http://twitter.com/user/status/135016568806166...   \n",
       "2  COVID-19  1.369750e+18  http://twitter.com/user/status/136974953915491...   \n",
       "3  COVID-19  1.350165e+18  http://twitter.com/user/status/135016499568693...   \n",
       "4  COVID-19  1.370008e+18  http://twitter.com/user/status/137000807648978...   \n",
       "\n",
       "                                          tweet_text         class_label  \n",
       "0  India's gift of 100,000 COVID-19 vaccines arri...  no_not_interesting  \n",
       "1  Here’s what I’m doing while I wait my turn for...  no_not_interesting  \n",
       "2  This afternoon, I’m hosting an event with the ...  no_not_interesting  \n",
       "3  Help shops like mine stay open. Mask up, avoid...  no_not_interesting  \n",
       "4  As part of the ongoing nationwide vaccination ...  no_not_interesting  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test shape: {}\\nTrain shape: {}\\nValidation shape: {}\".format(df_worthy_test.shape, df_worthy_train.shape, df_worthy_valid.shape))\n",
    "df_worthy_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling imbalanced multiclass text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['no_not_interesting' 'harmful' 'yes_calls_for_action'\n",
      " 'yes_blame_authorities' 'yes_discusses_cure' 'yes_discusses_action_taken'\n",
      " 'yes_asks_question' 'yes_contains_advice' 'yes_other']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Categories vs Number of Documents'}>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = df_worthy_train.class_label.unique()\n",
    "print('Categories: {}'.format(categories))\n",
    "df_worthy_train.class_label.value_counts().plot(kind='bar', title='Categories vs Number of Documents', cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_not_interesting            2851\n",
       "harmful                        173\n",
       "yes_blame_authorities          138\n",
       "yes_calls_for_action            48\n",
       "yes_discusses_cure              42\n",
       "yes_discusses_action_taken      27\n",
       "yes_other                       25\n",
       "yes_contains_advice             12\n",
       "yes_asks_question                5\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_worthy_train[\"class_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dfs = {}\n",
    "\n",
    "for label in df_worthy_train[\"class_label\"].unique():\n",
    "    label_dfs[label] = df_worthy_train[df_worthy_train[\"class_label\"] == label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_not_interesting            5\n",
       "harmful                       5\n",
       "yes_calls_for_action          5\n",
       "yes_blame_authorities         5\n",
       "yes_discusses_cure            5\n",
       "yes_discusses_action_taken    5\n",
       "yes_asks_question             5\n",
       "yes_contains_advice           5\n",
       "yes_other                     5\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_dfs = {}\n",
    "minority_class = min(df_worthy_train[\"class_label\"].value_counts())\n",
    "\n",
    "for key, val in label_dfs.items():\n",
    "    downsampled_dfs[\"downsampled_\" + key] = val.sample(n=minority_class)\n",
    "\n",
    "df_downsampled = pd.concat(downsampled_dfs.values())\n",
    "df_downsampled[\"class_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_not_interesting            2851\n",
       "harmful                       2851\n",
       "yes_calls_for_action          2851\n",
       "yes_blame_authorities         2851\n",
       "yes_discusses_cure            2851\n",
       "yes_discusses_action_taken    2851\n",
       "yes_asks_question             2851\n",
       "yes_contains_advice           2851\n",
       "yes_other                     2851\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled_dfs = {}\n",
    "majority_class = max(df_worthy_train[\"class_label\"].value_counts())\n",
    "\n",
    "for key, val in label_dfs.items():\n",
    "    upsampled_dfs[\"upsampled_\" + key] = val.sample(n=majority_class, replace=True)\n",
    "\n",
    "df_upsampled = pd.concat(upsampled_dfs.values())\n",
    "df_upsampled[\"class_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of over- and undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_not_interesting            369\n",
       "harmful                       369\n",
       "yes_calls_for_action          369\n",
       "yes_blame_authorities         369\n",
       "yes_discusses_cure            369\n",
       "yes_discusses_action_taken    369\n",
       "yes_asks_question             369\n",
       "yes_contains_advice           369\n",
       "yes_other                     369\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meansampled_dfs = {}\n",
    "mean_val = df_worthy_train[\"class_label\"].value_counts().mean()\n",
    "\n",
    "for key, val in label_dfs.items():\n",
    "    meansampled_dfs[\"meansampled\" + key] = val.sample(n=int(mean_val), replace=True)\n",
    "\n",
    "df_meansampled = pd.concat(meansampled_dfs.values())\n",
    "df_meansampled[\"class_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/how-to-handle-multiclass-imbalanced-data-say-no-to-smote-e9a7f393c310\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = list(class_weight.compute_class_weight(class_weight= 'balanced', \n",
    "                                                       classes = np.unique(df_worthy_train[\"class_label\"]), \n",
    "                                                       y = df_worthy_train[\"class_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_not_interesting            2851\n",
       "harmful                        173\n",
       "yes_blame_authorities          138\n",
       "yes_calls_for_action            48\n",
       "yes_discusses_cure              42\n",
       "yes_discusses_action_taken      27\n",
       "yes_other                       25\n",
       "yes_contains_advice             12\n",
       "yes_asks_question                5\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_worthy_train[\"class_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_worthy_train[\"class_label\"].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_not_interesting': 0.1294282707821817,\n",
       " 'harmful': 2.132947976878613,\n",
       " 'yes_blame_authorities': 2.6739130434782608,\n",
       " 'yes_calls_for_action': 7.6875,\n",
       " 'yes_discusses_cure': 8.785714285714286,\n",
       " 'yes_discusses_action_taken': 13.666666666666666,\n",
       " 'yes_other': 14.76,\n",
       " 'yes_contains_advice': 30.75,\n",
       " 'yes_asks_question': 73.8}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating weights\n",
    "class_weights.sort()\n",
    "class_weights\n",
    "\n",
    "df_worthy_train[\"class_label\"].value_counts()\n",
    "\n",
    "weights = {}\n",
    "for index, weight in enumerate(class_weights):\n",
    "    weights[labels[index]] = weight\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emojies when using the lemmatizer\n",
    "#https://poopcode.com/how-to-remove-emoji-from-text-in-python/\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, method=\"stopwords\"):\n",
    "    \n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Split sentence into words\n",
    "    tokens = []\n",
    "    for token in ngrams(text.split(), 1):\n",
    "        word = re.sub(r',', '', token[0]) #Remove commas\n",
    "        word = re.sub(r'[\\!\\.\\:]$', '', word) #Remove (.!:) at the end of tokens\n",
    "        word = re.sub(r'#', '', word) #Remove hashtags (not the text)\n",
    "        if word == \"—\": continue #Ignore dash\n",
    "        if word.find(\"@\") != -1: continue #Ignore tags\n",
    "        tokens.append(word)\n",
    "    tokens\n",
    "\n",
    "    # List comprehension to remove stopwords\n",
    "    if method != \"default\":\n",
    "        tokens = [x for x in tokens if x not in cachedStopWords]\n",
    "\n",
    "    # Perform stemming\n",
    "    if method == \"stemming\":\n",
    "        ps = PorterStemmer()\n",
    "        tokens = [ps.stem(x) for x in tokens]\n",
    "\n",
    "    # Perform lemmatization\n",
    "    if method == \"lemmatizer\":\n",
    "        # Remove emojies due to lemmatizer not handling them well\n",
    "        tokens = [x for x in tokens if remove_emoji(x) == x]\n",
    "        text = \" \".join(tokens)\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_normal(text):\n",
    "    return tokenize_text(text, method=\"default\")\n",
    "\n",
    "def tokenize_stopwords(text):\n",
    "    return tokenize_text(text, method=\"stopwords\")\n",
    "\n",
    "def tokenize_stemming(text):\n",
    "    return tokenize_text(text, method=\"stemming\")\n",
    "\n",
    "def tokenize_lemmatizer(text):\n",
    "    return tokenize_text(text, method=\"lemmatizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(df, text_col, method=\"default\"):\n",
    "    article_tokens = []\n",
    "    for i in range(len(df)):\n",
    "        text = df.iloc[i][text_col].lower()\n",
    "        article_tokens.append(tokenize_text(text, method))\n",
    "    df[\"tokens\"] = article_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_df(df_downsampled, \"tweet_text\", \"lemmatizer\")\n",
    "tokenize_df(df_upsampled, \"tweet_text\", \"lemmatizer\")\n",
    "tokenize_df(df_meansampled, \"tweet_text\", \"lemmatizer\")\n",
    "tokenize_df(df_worthy_train, \"tweet_text\", \"lemmatizer\")\n",
    "tokenize_df(df_worthy_test, \"tweet_text\", \"lemmatizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'transmissible', 'potentially', 'deadly', 'strain(s', ')', 'italy', 'lock', 'reminiscent', 'march', '2020', 'except', 'world', 'covid-19', 'wild', 'type', 'vaccine', 'mask', 'shield', 'distance', 'ventilation', 'hand', 'hygiene'] \n",
      "\n",
      "['here', 'be', 'late', 'update', 'vaccine', 'covid', 'cases', 'york', 'region', 'one', 'message', 'get', 'story', 'vaccinate', 'senior', 'must', 'remind', 'protect', '2', 'week', 'get', '-', '&', 'amp', ';', 'still', 'need', 'cautious'] \n",
      "\n",
      "['awful', 'brit', 'supply', 'lot', 'covid-19', 'vaccines.this', 'come', 'shortly', 'eu', 'admit', 'claim', 'uk', 'impose', \"'\", 'outright', 'ban', 'export', 'vaccine', \"'\", 'indeed', 'porky', 'pie', 'time', 'drop', 'brit', 'bash', 'agenda', 'work', 'together'] \n",
      "\n",
      "['india', \"'s\", 'gift', '100000', 'covid-19', 'vaccine', 'arrive', 'barbado', 'early', 'today', 'special', 'moment', 'barbadian', 'want', 'thank', 'prime', 'minister', 'modi', 'quick', 'decisive', 'magnanimous', 'action', 'allow', 'we', 'beneficiary', 'vaccine'] \n",
      "\n",
      "['senate', 'pass', 'covid', 'relief', '$', '1400', 'relief', 'check', 'fund', 'vaccine', 'money', 'reopen', 'school', 'food', 'unemployment', 'rental', 'assistance', 'cut', 'child', 'poverty', 'half', 'help', 'small', 'business', 'must', 'end', 'pandemic', 'help', 'way']\n"
     ]
    }
   ],
   "source": [
    "print(df_downsampled.iloc[0][\"tokens\"], \"\\n\")\n",
    "print(df_upsampled.iloc[0][\"tokens\"], \"\\n\")\n",
    "print(df_meansampled.iloc[0][\"tokens\"], \"\\n\")\n",
    "print(df_worthy_train.iloc[0][\"tokens\"], \"\\n\")\n",
    "print(df_worthy_test.iloc[0][\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # For creating a DTM (discrete values)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # A weighted DTM (fractions)\n",
    "\n",
    "cv_dict = {}\n",
    "tfidf_dict = {}\n",
    "ngram_list = [1,2,3]\n",
    "for n in ngram_list:\n",
    "    cv_dict[n] = CountVectorizer(tokenizer=tokenize_lemmatizer, ngram_range=(n, n), max_features=5000)\n",
    "    tfidf_dict[n] = TfidfVectorizer(tokenizer=tokenize_stopwords, ngram_range=(n, n), max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Emil\\DAT550\\Final_Project\\1d.ipynb Cell 34'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000031?line=23'>24</a>\u001b[0m tweet_bow_test_d[n] \u001b[39m=\u001b[39m tweet_vec_d[n]\u001b[39m.\u001b[39mtransform(df_worthy_test[\u001b[39m\"\u001b[39m\u001b[39mtweet_text\u001b[39m\u001b[39m\"\u001b[39m]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000031?line=25'>26</a>\u001b[0m \u001b[39m# balanced upsample\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000031?line=26'>27</a>\u001b[0m tweet_vec_u[n] \u001b[39m=\u001b[39m cv_dict[n]\u001b[39m.\u001b[39;49mfit(df_upsampled[\u001b[39m\"\u001b[39;49m\u001b[39mtweet_text\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \u001b[39m# DTM (CV, but normalized for relative frequency)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000031?line=27'>28</a>\u001b[0m tweet_bow_train_u[n] \u001b[39m=\u001b[39m tweet_vec_u[n]\u001b[39m.\u001b[39mtransform(df_upsampled[\u001b[39m\"\u001b[39m\u001b[39mtweet_text\u001b[39m\u001b[39m\"\u001b[39m]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000031?line=28'>29</a>\u001b[0m tweet_bow_test_u[n] \u001b[39m=\u001b[39m tweet_vec_u[n]\u001b[39m.\u001b[39mtransform(df_worthy_test[\u001b[39m\"\u001b[39m\u001b[39mtweet_text\u001b[39m\u001b[39m\"\u001b[39m]) \n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1283\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1266'>1267</a>\u001b[0m \u001b[39m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1267'>1268</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1268'>1269</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1279'>1280</a>\u001b[0m \u001b[39m    Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1280'>1281</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1281'>1282</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m-> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1282'>1283</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1283'>1284</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1321'>1322</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1322'>1323</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1323'>1324</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1324'>1325</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1325'>1326</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1326'>1327</a>\u001b[0m             )\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1327'>1328</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1329'>1330</a>\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1331'>1332</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1332'>1333</a>\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1198'>1199</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1199'>1200</a>\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1200'>1201</a>\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1201'>1202</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1202'>1203</a>\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:115\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=112'>113</a>\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=114'>115</a>\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=115'>116</a>\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=116'>117</a>\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Users\\Emil\\DAT550\\Final_Project\\1d.ipynb Cell 27'\u001b[0m in \u001b[0;36mtokenize_lemmatizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000025?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_lemmatizer\u001b[39m(text):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000025?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenize_text(text, method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlemmatizer\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Emil\\DAT550\\Final_Project\\1d.ipynb Cell 26'\u001b[0m in \u001b[0;36mtokenize_text\u001b[1;34m(text, method)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000024?line=28'>29</a>\u001b[0m     tokens \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m remove_emoji(x) \u001b[39m==\u001b[39m x]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000024?line=29'>30</a>\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000024?line=30'>31</a>\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000024?line=31'>32</a>\u001b[0m     tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000024?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:1017\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1014'>1015</a>\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1015'>1016</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1016'>1017</a>\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1017'>1018</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1018'>1019</a>\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1019'>1020</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:250\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:265\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=310'>311</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=311'>312</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=312'>313</a>\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=313'>314</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=314'>315</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\ml\\tb_framework.py:33\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model, X, is_train):\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=32'>33</a>\u001b[0m     step_model \u001b[39m=\u001b[39m ParserStepModel(\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=33'>34</a>\u001b[0m         X,\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=34'>35</a>\u001b[0m         model\u001b[39m.\u001b[39;49mlayers,\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=35'>36</a>\u001b[0m         unseen_classes\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39munseen_classes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=36'>37</a>\u001b[0m         train\u001b[39m=\u001b[39;49mis_train,\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=37'>38</a>\u001b[0m         has_upper\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39mhas_upper\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=38'>39</a>\u001b[0m     )\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=40'>41</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m step_model, step_model\u001b[39m.\u001b[39mfinish_steps\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\ml\\parser_model.pyx:216\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=51'>52</a>\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=53'>54</a>\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=54'>55</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=55'>56</a>\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=51'>52</a>\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=53'>54</a>\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=54'>55</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=55'>56</a>\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "    \u001b[1;31m[... skipping similar frames: Model.__call__ at line 291 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=51'>52</a>\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=53'>54</a>\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=54'>55</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=55'>56</a>\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\with_array.py:30\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[39mbool\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=28'>29</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Ragged):\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=29'>30</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _ragged_forward(\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=30'>31</a>\u001b[0m             cast(Model[Ragged, Ragged], model), cast(Ragged, Xseq), is_train\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=31'>32</a>\u001b[0m         )\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=32'>33</a>\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Padded):\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=33'>34</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _padded_forward(\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=34'>35</a>\u001b[0m             cast(Model[Padded, Padded], model), cast(Padded, Xseq), is_train\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=35'>36</a>\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\with_array.py:90\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=85'>86</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ragged_forward\u001b[39m(\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=86'>87</a>\u001b[0m     model: Model[Ragged, Ragged], Xr: Ragged, is_train: \u001b[39mbool\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=87'>88</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=88'>89</a>\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=89'>90</a>\u001b[0m     Y, get_dX \u001b[39m=\u001b[39m layer(Xr\u001b[39m.\u001b[39;49mdataXd, is_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=91'>92</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYr: Ragged) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Ragged:\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=92'>93</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[39m.\u001b[39mdataXd), dYr\u001b[39m.\u001b[39mlengths)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=51'>52</a>\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=53'>54</a>\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=54'>55</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=55'>56</a>\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=51'>52</a>\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=53'>54</a>\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=54'>55</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=55'>56</a>\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\maxout.py:49\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/maxout.py?line=46'>47</a>\u001b[0m W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_param(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/maxout.py?line=47'>48</a>\u001b[0m W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape2f(W, nO \u001b[39m*\u001b[39m nP, nI)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/maxout.py?line=48'>49</a>\u001b[0m Y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mgemm(X, W, trans2\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/maxout.py?line=49'>50</a>\u001b[0m Y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape1f(b, nO \u001b[39m*\u001b[39m nP)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/maxout.py?line=50'>51</a>\u001b[0m Z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape3f(Y, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], nO, nP)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tweet_vec = {}\n",
    "tweet_bow_train = {}\n",
    "tweet_bow_test = {}\n",
    "\n",
    "tweet_vec_d = {}\n",
    "tweet_bow_train_d = {}\n",
    "tweet_bow_test_d = {}\n",
    "\n",
    "tweet_vec_u = {}\n",
    "tweet_bow_train_u = {}\n",
    "tweet_bow_test_u = {}\n",
    "\n",
    "tweet_vec_h = {}\n",
    "tweet_bow_train_h = {}\n",
    "tweet_bow_test_h = {}\n",
    "for n in ngram_list:\n",
    "    tweet_vec[n] = cv_dict[n].fit(df_worthy_train[\"tweet_text\"]) # DTM (CV, but normalized for relative frequency)\n",
    "    tweet_bow_train[n] = tweet_vec[n].transform(df_worthy_train[\"tweet_text\"]) \n",
    "    tweet_bow_test[n] = tweet_vec[n].transform(df_worthy_test[\"tweet_text\"]) \n",
    "    \n",
    "    # balanced downsample\n",
    "    tweet_vec_d[n] = cv_dict[n].fit(df_downsampled[\"tweet_text\"]) # DTM (CV, but normalized for relative frequency)\n",
    "    tweet_bow_train_d[n] = tweet_vec_d[n].transform(df_downsampled[\"tweet_text\"]) \n",
    "    tweet_bow_test_d[n] = tweet_vec_d[n].transform(df_worthy_test[\"tweet_text\"]) \n",
    "    \n",
    "    # balanced upsample\n",
    "    tweet_vec_u[n] = cv_dict[n].fit(df_upsampled[\"tweet_text\"]) # DTM (CV, but normalized for relative frequency)\n",
    "    tweet_bow_train_u[n] = tweet_vec_u[n].transform(df_upsampled[\"tweet_text\"]) \n",
    "    tweet_bow_test_u[n] = tweet_vec_u[n].transform(df_worthy_test[\"tweet_text\"]) \n",
    "    \n",
    "    # balanced hybridsample\n",
    "    tweet_vec_h[n] = cv_dict[n].fit(df_meansampled[\"tweet_text\"]) # DTM (CV, but normalized for relative frequency)\n",
    "    tweet_bow_train_h[n] = tweet_vec_h[n].transform(df_meansampled[\"tweet_text\"]) \n",
    "    tweet_bow_test_h[n] = tweet_vec_h[n].transform(df_worthy_test[\"tweet_text\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Unbalanced--------\n",
      "1-gram Priors: [0.05209274 0.85847636 0.00150557 0.04155375 0.01445348 0.00361337\n",
      " 0.00813008 0.01264679 0.00752785]\n",
      "Dummy F1-Score for 1-gram: 91.98%\n",
      "Dummy Precision-Score for 1-gram: 11.11%\n",
      "Dummy Precision-Score for 1-gram: 9.46%\n",
      "--------Downsampled--------\n",
      "1-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 1-gram: 11.41%\n",
      "Dummy Precision-Score for 1-gram: 11.11%\n",
      "Dummy Precision-Score for 1-gram: 0.67%\n",
      "--------Upsampled--------\n",
      "1-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 1-gram: 11.41%\n",
      "Dummy Precision-Score for 1-gram: 11.11%\n",
      "Dummy Precision-Score for 1-gram: 0.67%\n",
      "--------Meansampled--------\n",
      "1-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 1-gram: 11.41%\n",
      "Dummy Precision-Score for 1-gram: 11.11%\n",
      "Dummy Precision-Score for 1-gram: 0.67%\n",
      "--------------------------------------\n",
      "--------Unbalanced--------\n",
      "2-gram Priors: [0.05209274 0.85847636 0.00150557 0.04155375 0.01445348 0.00361337\n",
      " 0.00813008 0.01264679 0.00752785]\n",
      "Dummy F1-Score for 2-gram: 91.98%\n",
      "Dummy Precision-Score for 2-gram: 11.11%\n",
      "Dummy Precision-Score for 2-gram: 9.46%\n",
      "--------Downsampled--------\n",
      "2-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 2-gram: 11.41%\n",
      "Dummy Precision-Score for 2-gram: 11.11%\n",
      "Dummy Precision-Score for 2-gram: 0.67%\n",
      "--------Upsampled--------\n",
      "2-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 2-gram: 11.41%\n",
      "Dummy Precision-Score for 2-gram: 11.11%\n",
      "Dummy Precision-Score for 2-gram: 0.67%\n",
      "--------Meansampled--------\n",
      "2-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 2-gram: 11.41%\n",
      "Dummy Precision-Score for 2-gram: 11.11%\n",
      "Dummy Precision-Score for 2-gram: 0.67%\n",
      "--------------------------------------\n",
      "--------Unbalanced--------\n",
      "3-gram Priors: [0.05209274 0.85847636 0.00150557 0.04155375 0.01445348 0.00361337\n",
      " 0.00813008 0.01264679 0.00752785]\n",
      "Dummy F1-Score for 3-gram: 91.98%\n",
      "Dummy Precision-Score for 3-gram: 11.11%\n",
      "Dummy Precision-Score for 3-gram: 9.46%\n",
      "--------Downsampled--------\n",
      "3-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 3-gram: 11.41%\n",
      "Dummy Precision-Score for 3-gram: 11.11%\n",
      "Dummy Precision-Score for 3-gram: 0.67%\n",
      "--------Upsampled--------\n",
      "3-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 3-gram: 11.41%\n",
      "Dummy Precision-Score for 3-gram: 11.11%\n",
      "Dummy Precision-Score for 3-gram: 0.67%\n",
      "--------Meansampled--------\n",
      "3-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 3-gram: 11.41%\n",
      "Dummy Precision-Score for 3-gram: 11.11%\n",
      "Dummy Precision-Score for 3-gram: 0.67%\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "for n in ngram_list:\n",
    "    print(\"--------Unbalanced--------\")\n",
    "    dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "    dummy_clf.fit(tweet_bow_train[n], df_worthy_train[\"class_label\"])\n",
    "    print(\"{}-gram Priors: {}\".format(n, dummy_clf.class_prior_))\n",
    "\n",
    "    f1_score = metrics.f1_score(dummy_clf.predict(tweet_bow_test[n]), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Dummy F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(dummy_clf.predict(tweet_bow_test[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(dummy_clf.predict(tweet_bow_test[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------Downsampled--------\")\n",
    "    dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "    dummy_clf.fit(tweet_bow_train_d[n], df_downsampled[\"class_label\"])\n",
    "    print(\"{}-gram Priors: {}\".format(n, dummy_clf.class_prior_))\n",
    "\n",
    "    f1_score = metrics.f1_score(dummy_clf.predict(tweet_bow_test_d[n]), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Dummy F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(dummy_clf.predict(tweet_bow_test_d[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(dummy_clf.predict(tweet_bow_test_d[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------Upsampled--------\")\n",
    "    dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "    dummy_clf.fit(tweet_bow_train_u[n], df_upsampled[\"class_label\"])\n",
    "    print(\"{}-gram Priors: {}\".format(n, dummy_clf.class_prior_))\n",
    "\n",
    "    f1_score = metrics.f1_score(dummy_clf.predict(tweet_bow_test_u[n]), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Dummy F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(dummy_clf.predict(tweet_bow_test_u[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(dummy_clf.predict(tweet_bow_test_u[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------Meansampled--------\")\n",
    "    dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "    dummy_clf.fit(tweet_bow_train_h[n], df_meansampled[\"class_label\"])\n",
    "    print(\"{}-gram Priors: {}\".format(n, dummy_clf.class_prior_))\n",
    "\n",
    "    f1_score = metrics.f1_score(dummy_clf.predict(tweet_bow_test_h[n]), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Dummy F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(dummy_clf.predict(tweet_bow_test_h[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(dummy_clf.predict(tweet_bow_test_h[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_w = LogisticRegression(class_weight=weights)\n",
    "lr_n = LogisticRegression(class_weight=None)\n",
    "\n",
    "lr_d = LogisticRegression(class_weight=None)\n",
    "lr_u = LogisticRegression(class_weight=None)\n",
    "lr_h = LogisticRegression(class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Custom weight--------\n",
      "Logistic Regression F1-Score for 1-gram: 80.03%\n",
      "Logistic Regression Precision-Score for 1-gram: 26.5%\n",
      "Logistic Regression Recall-Score for 1-gram: 23.42%\n",
      "--------No weight--------\n",
      "Logistic Regression F1-Score for 1-gram: 88.1%\n",
      "Logistic Regression Precision-Score for 1-gram: 19.58%\n",
      "Logistic Regression Recall-Score for 1-gram: 34.89%\n",
      "--------------------------------------\n",
      "--------Custom weight--------\n",
      "Logistic Regression F1-Score for 2-gram: 79.3%\n",
      "Logistic Regression Precision-Score for 2-gram: 24.46%\n",
      "Logistic Regression Recall-Score for 2-gram: 20.57%\n",
      "--------No weight--------\n",
      "Logistic Regression F1-Score for 2-gram: 90.88%\n",
      "Logistic Regression Precision-Score for 2-gram: 14.15%\n",
      "Logistic Regression Recall-Score for 2-gram: 26.52%\n",
      "--------------------------------------\n",
      "--------Custom weight--------\n",
      "Logistic Regression F1-Score for 3-gram: 80.43%\n",
      "Logistic Regression Precision-Score for 3-gram: 16.26%\n",
      "Logistic Regression Recall-Score for 3-gram: 19.5%\n",
      "--------No weight--------\n",
      "Logistic Regression F1-Score for 3-gram: 91.85%\n",
      "Logistic Regression Precision-Score for 3-gram: 11.92%\n",
      "Logistic Regression Recall-Score for 3-gram: 20.61%\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n in ngram_list:\n",
    "    lr_w.fit(tweet_bow_train[n].toarray(), df_worthy_train[\"class_label\"])\n",
    "    lr_n.fit(tweet_bow_train[n].toarray(), df_worthy_train[\"class_label\"])\n",
    "    \n",
    "    print(\"--------Custom weight--------\")\n",
    "    f1_score = metrics.f1_score(lr_w.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Logistic Regression F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(lr_w.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(lr_w.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------No weight--------\")\n",
    "    f1_score = metrics.f1_score(lr_n.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Logistic Regression F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(lr_n.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(lr_n.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Downsampled--------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [909, 45]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17960/1586547565.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--------Downsampled--------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mf1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_bow_test_d\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_downsampled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"class_label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"weighted\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Logistic Regression F1-Score for {}-gram: {}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.66666667\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m        \u001b[1;33m,\u001b[0m \u001b[1;36m0.66666667\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \"\"\"\n\u001b[1;32m-> 1132\u001b[1;33m     return fbeta_score(\n\u001b[0m\u001b[0;32m   1133\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1268\u001b[0m     \"\"\"\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m     _, _, f, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[0;32m   1271\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1554\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1556\u001b[1;33m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1558\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"average has to be one of \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1357\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1358\u001b[0m     \u001b[1;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_pred\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    384\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [909, 45]"
     ]
    }
   ],
   "source": [
    "for n in ngram_list:\n",
    "    lr_d.fit(tweet_bow_train_d[n].toarray(), df_downsampled[\"class_label\"])\n",
    "    lr_u.fit(tweet_bow_train_u[n].toarray(), df_upsampled[\"class_label\"])\n",
    "    lr_h.fit(tweet_bow_train_h[n].toarray(), df_meansampled[\"class_label\"])\n",
    "    \n",
    "    # print(\"--------Downsampled--------\")\n",
    "    # f1_score = metrics.f1_score(lr_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"Logistic Regression F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(lr_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"Logistic Regression Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(lr_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"Logistic Regression Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------Upsampled--------\")\n",
    "    f1_score = metrics.f1_score(lr_u.predict(tweet_bow_test_u[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Logistic Regression F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(lr_u.predict(tweet_bow_test_u[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(lr_u.predict(tweet_bow_test_u[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------Meansampled--------\")\n",
    "    f1_score = metrics.f1_score(lr_h.predict(tweet_bow_test_h[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Logistic Regression F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(lr_h.predict(tweet_bow_test_h[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(lr_h.predict(tweet_bow_test_h[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://iq.opengenus.org/text-classification-using-k-nearest-neighbors/\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9)#, weights=class_weights)\n",
    "knn_d = KNeighborsClassifier(n_neighbors=9)#, weights=class_weights)\n",
    "knn_u = KNeighborsClassifier(n_neighbors=9)#, weights=class_weights)\n",
    "knn_h = KNeighborsClassifier(n_neighbors=9)#, weights=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Unbalanced--------\n",
      "KNN F1-Score for 1-gram: 91.98%\n",
      "KNN Precision-Score for 1-gram: 11.11%\n",
      "KNN Recall-Score for 1-gram: 9.46%\n",
      "--------Unbalanced--------\n",
      "KNN F1-Score for 2-gram: 91.98%\n",
      "KNN Precision-Score for 2-gram: 11.11%\n",
      "KNN Recall-Score for 2-gram: 9.46%\n",
      "--------Unbalanced--------\n",
      "KNN F1-Score for 3-gram: 91.65%\n",
      "KNN Precision-Score for 3-gram: 11.08%\n",
      "KNN Recall-Score for 3-gram: 9.46%\n"
     ]
    }
   ],
   "source": [
    "for n in ngram_list:\n",
    "    knn.fit(tweet_bow_train[n].toarray(), df_worthy_train[\"class_label\"])\n",
    "    knn_d.fit(tweet_bow_train_d[n].toarray(), df_downsampled[\"class_label\"])\n",
    "    knn_u.fit(tweet_bow_train_u[n].toarray(), df_upsampled[\"class_label\"])\n",
    "    knn_h.fit(tweet_bow_train_h[n].toarray(), df_meansampled[\"class_label\"])\n",
    "    \n",
    "    print(\"--------Unbalanced--------\")\n",
    "    f1_score = metrics.f1_score(knn.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(knn.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(knn.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Downsampled--------\")\n",
    "    # f1_score = metrics.f1_score(knn_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(knn_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(knn_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Upsampled--------\")\n",
    "    # f1_score = metrics.f1_score(knn_u.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(knn_u.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(knn_u.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Meansampled--------\")\n",
    "    # f1_score = metrics.f1_score(knn_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(knn_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(knn_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    # print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Unbalanced--------\n",
      "Complement F1-Score for 1-gram: 90.04%\n",
      "Complement precision for 1-gram: 13.09%\n",
      "Complement recall for 1-gram: 18.76%\n",
      "--------Unbalanced--------\n",
      "Complement F1-Score for 2-gram: 90.74%\n",
      "Complement precision for 2-gram: 14.23%\n",
      "Complement recall for 2-gram: 26.29%\n",
      "--------Unbalanced--------\n",
      "Complement F1-Score for 3-gram: 91.5%\n",
      "Complement precision for 3-gram: 12.2%\n",
      "Complement recall for 3-gram: 23.95%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # suited for classification with discrete features\n",
    "from sklearn.naive_bayes import CategoricalNB \n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf_d = MultinomialNB()\n",
    "clf_u = MultinomialNB()\n",
    "clf_h = MultinomialNB()\n",
    "\n",
    "for n in ngram_list:\n",
    "    clf.fit(tweet_bow_train[n].toarray(), df_worthy_train[\"class_label\"])\n",
    "    clf_d.fit(tweet_bow_train_d[n].toarray(), df_downsampled[\"class_label\"])\n",
    "    clf_u.fit(tweet_bow_train_u[n].toarray(), df_upsampled[\"class_label\"])\n",
    "    clf_h.fit(tweet_bow_train_h[n].toarray(), df_meansampled[\"class_label\"])\n",
    "    \n",
    "    print(\"--------Unbalanced--------\")\n",
    "    f1_score = metrics.f1_score(clf.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Complement F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(clf.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Complement precision for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(clf.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Complement recall for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Downsampled--------\")\n",
    "    # f1_score = metrics.f1_score(clf_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(clf_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(clf_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Upsampled--------\")\n",
    "    # f1_score = metrics.f1_score(clf_u.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"Complement F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(clf_u.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"Complement precision for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(clf.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"Complement recall for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Meansampled--------\")\n",
    "    # f1_score = metrics.f1_score(clf_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(clf_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(clf_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    # print(\"--------------------------------------\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "30f714e597357a060011314c390f095b14bdfc77e179572e10be14d1253a6501"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
