{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1d) Check whether a tweet is attention-worthy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/ivarw/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import spacy\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\") # Ignore warnings\n",
    "nltk.download('stopwords') # Load Stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\") # Load Lemmatizer\n",
    "cachedStopWords = list(set(stopwords.words(\"english\")))\n",
    "\n",
    "# Twitter Datasets\n",
    "df_worthy_train = pd.read_csv(\"data/1d/CT22_english_1D_attentionworthy_train.tsv\", sep='\\t')\n",
    "df_worthy_valid = pd.read_csv(\"data/1d/CT22_english_1D_attentionworthy_dev.tsv\", sep='\\t')\n",
    "df_worthy_test = pd.read_csv(\"data/1d/CT22_english_1D_attentionworthy_dev_test.tsv\", sep='\\t')\n",
    "\n",
    "# Top 3000 most commonly used english words + covid related medical terms (used as vocabulary for the vectorization process)\n",
    "topwords = pd.read_csv('words.txt', sep=\" \", header=None).values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test shape: (909, 5)\n",
      "Train shape: (3321, 5)\n",
      "Validation shape: (306, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>class_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>1.359351e+18</td>\n",
       "      <td>http://twitter.com/user/status/135935094335617...</td>\n",
       "      <td>India's gift of 100,000 COVID-19 vaccines arri...</td>\n",
       "      <td>no_not_interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>1.350166e+18</td>\n",
       "      <td>http://twitter.com/user/status/135016568806166...</td>\n",
       "      <td>Here’s what I’m doing while I wait my turn for...</td>\n",
       "      <td>no_not_interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>1.369750e+18</td>\n",
       "      <td>http://twitter.com/user/status/136974953915491...</td>\n",
       "      <td>This afternoon, I’m hosting an event with the ...</td>\n",
       "      <td>no_not_interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>1.350165e+18</td>\n",
       "      <td>http://twitter.com/user/status/135016499568693...</td>\n",
       "      <td>Help shops like mine stay open. Mask up, avoid...</td>\n",
       "      <td>no_not_interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>COVID-19</td>\n",
       "      <td>1.370008e+18</td>\n",
       "      <td>http://twitter.com/user/status/137000807648978...</td>\n",
       "      <td>As part of the ongoing nationwide vaccination ...</td>\n",
       "      <td>no_not_interesting</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      topic      tweet_id                                          tweet_url  \\\n",
       "0  COVID-19  1.359351e+18  http://twitter.com/user/status/135935094335617...   \n",
       "1  COVID-19  1.350166e+18  http://twitter.com/user/status/135016568806166...   \n",
       "2  COVID-19  1.369750e+18  http://twitter.com/user/status/136974953915491...   \n",
       "3  COVID-19  1.350165e+18  http://twitter.com/user/status/135016499568693...   \n",
       "4  COVID-19  1.370008e+18  http://twitter.com/user/status/137000807648978...   \n",
       "\n",
       "                                          tweet_text         class_label  \n",
       "0  India's gift of 100,000 COVID-19 vaccines arri...  no_not_interesting  \n",
       "1  Here’s what I’m doing while I wait my turn for...  no_not_interesting  \n",
       "2  This afternoon, I’m hosting an event with the ...  no_not_interesting  \n",
       "3  Help shops like mine stay open. Mask up, avoid...  no_not_interesting  \n",
       "4  As part of the ongoing nationwide vaccination ...  no_not_interesting  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Test shape: {}\\nTrain shape: {}\\nValidation shape: {}\".format(df_worthy_test.shape, df_worthy_train.shape, df_worthy_valid.shape))\n",
    "df_worthy_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling imbalanced multiclass text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories: ['no_not_interesting' 'harmful' 'yes_calls_for_action'\n",
      " 'yes_blame_authorities' 'yes_discusses_cure' 'yes_discusses_action_taken'\n",
      " 'yes_asks_question' 'yes_contains_advice' 'yes_other']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Categories vs Number of Documents'}>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "categories = df_worthy_train.class_label.unique()\n",
    "print('Categories: {}'.format(categories))\n",
    "df_worthy_train.class_label.value_counts().plot(kind='bar', title='Categories vs Number of Documents', cmap='plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_not_interesting            2851\n",
       "harmful                        173\n",
       "yes_blame_authorities          138\n",
       "yes_calls_for_action            48\n",
       "yes_discusses_cure              42\n",
       "yes_discusses_action_taken      27\n",
       "yes_other                       25\n",
       "yes_contains_advice             12\n",
       "yes_asks_question                5\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_worthy_train[\"class_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dfs = {}\n",
    "\n",
    "for label in df_worthy_train[\"class_label\"].unique():\n",
    "    label_dfs[label] = df_worthy_train[df_worthy_train[\"class_label\"] == label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_not_interesting            5\n",
       "harmful                       5\n",
       "yes_calls_for_action          5\n",
       "yes_blame_authorities         5\n",
       "yes_discusses_cure            5\n",
       "yes_discusses_action_taken    5\n",
       "yes_asks_question             5\n",
       "yes_contains_advice           5\n",
       "yes_other                     5\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "downsampled_dfs = {}\n",
    "minority_class = min(df_worthy_train[\"class_label\"].value_counts())\n",
    "\n",
    "for key, val in label_dfs.items():\n",
    "    downsampled_dfs[\"downsampled_\" + key] = val.sample(n=minority_class)\n",
    "\n",
    "df_downsampled = pd.concat(downsampled_dfs.values())\n",
    "df_downsampled[\"class_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_not_interesting            2851\n",
       "harmful                       2851\n",
       "yes_calls_for_action          2851\n",
       "yes_blame_authorities         2851\n",
       "yes_discusses_cure            2851\n",
       "yes_discusses_action_taken    2851\n",
       "yes_asks_question             2851\n",
       "yes_contains_advice           2851\n",
       "yes_other                     2851\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upsampled_dfs = {}\n",
    "majority_class = max(df_worthy_train[\"class_label\"].value_counts())\n",
    "\n",
    "for key, val in label_dfs.items():\n",
    "    upsampled_dfs[\"upsampled_\" + key] = val.sample(n=majority_class, replace=True)\n",
    "\n",
    "df_upsampled = pd.concat(upsampled_dfs.values())\n",
    "df_upsampled[\"class_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combination of over- and undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_not_interesting            369\n",
       "harmful                       369\n",
       "yes_calls_for_action          369\n",
       "yes_blame_authorities         369\n",
       "yes_discusses_cure            369\n",
       "yes_discusses_action_taken    369\n",
       "yes_asks_question             369\n",
       "yes_contains_advice           369\n",
       "yes_other                     369\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meansampled_dfs = {}\n",
    "mean_val = df_worthy_train[\"class_label\"].value_counts().mean()\n",
    "\n",
    "for key, val in label_dfs.items():\n",
    "    meansampled_dfs[\"meansampled\" + key] = val.sample(n=int(mean_val), replace=True)\n",
    "\n",
    "df_meansampled = pd.concat(meansampled_dfs.values())\n",
    "df_meansampled[\"class_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/how-to-handle-multiclass-imbalanced-data-say-no-to-smote-e9a7f393c310\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = list(class_weight.compute_class_weight(class_weight= 'balanced', \n",
    "                                                       classes = np.unique(df_worthy_train[\"class_label\"]), \n",
    "                                                       y = df_worthy_train[\"class_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "no_not_interesting            2851\n",
       "harmful                        173\n",
       "yes_blame_authorities          138\n",
       "yes_calls_for_action            48\n",
       "yes_discusses_cure              42\n",
       "yes_discusses_action_taken      27\n",
       "yes_other                       25\n",
       "yes_contains_advice             12\n",
       "yes_asks_question                5\n",
       "Name: class_label, dtype: int64"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_worthy_train[\"class_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_worthy_train[\"class_label\"].value_counts().index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'no_not_interesting': 0.1294282707821817,\n",
       " 'harmful': 2.132947976878613,\n",
       " 'yes_blame_authorities': 2.6739130434782608,\n",
       " 'yes_calls_for_action': 7.6875,\n",
       " 'yes_discusses_cure': 8.785714285714286,\n",
       " 'yes_discusses_action_taken': 13.666666666666666,\n",
       " 'yes_other': 14.76,\n",
       " 'yes_contains_advice': 30.75,\n",
       " 'yes_asks_question': 73.8}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating weights\n",
    "class_weights.sort()\n",
    "class_weights\n",
    "\n",
    "df_worthy_train[\"class_label\"].value_counts()\n",
    "\n",
    "weights = {}\n",
    "for index, weight in enumerate(class_weights):\n",
    "    weights[labels[index]] = weight\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove emojies when using the lemmatizer\n",
    "#https://poopcode.com/how-to-remove-emoji-from-text-in-python/\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                               u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  # dingbats\n",
    "                               u\"\\u3030\"\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text, method=\"stopwords\"):\n",
    "    \n",
    "    # Remove hyperlinks\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "    # Split sentence into words\n",
    "    tokens = []\n",
    "    for token in ngrams(text.split(), 1):\n",
    "        word = re.sub(r',', '', token[0]) #Remove commas\n",
    "        word = re.sub(r'[\\!\\.\\:]$', '', word) #Remove (.!:) at the end of tokens\n",
    "        word = re.sub(r'#', '', word) #Remove hashtags (not the text)\n",
    "        if word == \"—\": continue #Ignore dash\n",
    "        if word.find(\"@\") != -1: continue #Ignore tags\n",
    "        tokens.append(word)\n",
    "    tokens\n",
    "\n",
    "    # List comprehension to remove stopwords\n",
    "    if method != \"default\":\n",
    "        tokens = [x for x in tokens if x not in cachedStopWords]\n",
    "\n",
    "    # Perform stemming\n",
    "    if method == \"stemming\":\n",
    "        ps = PorterStemmer()\n",
    "        tokens = [ps.stem(x) for x in tokens]\n",
    "\n",
    "    # Perform lemmatization\n",
    "    if method == \"lemmatizer\":\n",
    "        # Remove emojies due to lemmatizer not handling them well\n",
    "        tokens = [x for x in tokens if remove_emoji(x) == x]\n",
    "        text = \" \".join(tokens)\n",
    "        doc = nlp(text)\n",
    "        tokens = [token.lemma_ for token in doc]\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_normal(text):\n",
    "    return tokenize_text(text, method=\"default\")\n",
    "\n",
    "def tokenize_stopwords(text):\n",
    "    return tokenize_text(text, method=\"stopwords\")\n",
    "\n",
    "def tokenize_stemming(text):\n",
    "    return tokenize_text(text, method=\"stemming\")\n",
    "\n",
    "def tokenize_lemmatizer(text):\n",
    "    return tokenize_text(text, method=\"lemmatizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_df(df, text_col, method=\"default\"):\n",
    "    article_tokens = []\n",
    "    for i in range(len(df)):\n",
    "        text = df.iloc[i][text_col].lower()\n",
    "        article_tokens.append(tokenize_text(text, method))\n",
    "    df[\"tokens\"] = article_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_df(df_downsampled, \"tweet_text\", \"lemmatizer\")\n",
    "tokenize_df(df_upsampled, \"tweet_text\", \"lemmatizer\")\n",
    "tokenize_df(df_meansampled, \"tweet_text\", \"lemmatizer\")\n",
    "tokenize_df(df_worthy_train, \"tweet_text\", \"lemmatizer\")\n",
    "tokenize_df(df_worthy_test, \"tweet_text\", \"lemmatizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['race', 'transmissible', 'potentially', 'deadly', 'strain(s', ')', 'italy', 'lock', 'reminiscent', 'march', '2020', 'except', 'world', 'covid-19', 'wild', 'type', 'vaccine', 'mask', 'shield', 'distance', 'ventilation', 'hand', 'hygiene'] \n",
      "\n",
      "['here', 'be', 'late', 'update', 'vaccine', 'covid', 'cases', 'york', 'region', 'one', 'message', 'get', 'story', 'vaccinate', 'senior', 'must', 'remind', 'protect', '2', 'week', 'get', '-', '&', 'amp', ';', 'still', 'need', 'cautious'] \n",
      "\n",
      "['awful', 'brit', 'supply', 'lot', 'covid-19', 'vaccines.this', 'come', 'shortly', 'eu', 'admit', 'claim', 'uk', 'impose', \"'\", 'outright', 'ban', 'export', 'vaccine', \"'\", 'indeed', 'porky', 'pie', 'time', 'drop', 'brit', 'bash', 'agenda', 'work', 'together'] \n",
      "\n",
      "['india', \"'s\", 'gift', '100000', 'covid-19', 'vaccine', 'arrive', 'barbado', 'early', 'today', 'special', 'moment', 'barbadian', 'want', 'thank', 'prime', 'minister', 'modi', 'quick', 'decisive', 'magnanimous', 'action', 'allow', 'we', 'beneficiary', 'vaccine'] \n",
      "\n",
      "['senate', 'pass', 'covid', 'relief', '$', '1400', 'relief', 'check', 'fund', 'vaccine', 'money', 'reopen', 'school', 'food', 'unemployment', 'rental', 'assistance', 'cut', 'child', 'poverty', 'half', 'help', 'small', 'business', 'must', 'end', 'pandemic', 'help', 'way']\n"
     ]
    }
   ],
   "source": [
    "print(df_downsampled.iloc[0][\"tokens\"], \"\\n\")\n",
    "print(df_upsampled.iloc[0][\"tokens\"], \"\\n\")\n",
    "print(df_meansampled.iloc[0][\"tokens\"], \"\\n\")\n",
    "print(df_worthy_train.iloc[0][\"tokens\"], \"\\n\")\n",
    "print(df_worthy_test.iloc[0][\"tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer # For creating a DTM (discrete values)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # A weighted DTM (fractions)\n",
    "\n",
    "cv_dict = {}\n",
    "tfidf_dict = {}\n",
    "ngram_list = [1,2,3]\n",
    "for n in ngram_list:\n",
    "    cv_dict[n] = CountVectorizer(tokenizer=tokenize_lemmatizer, ngram_range=(n, n), max_features=5000)\n",
    "    tfidf_dict[n] = TfidfVectorizer(tokenizer=tokenize_stopwords, ngram_range=(n, n), max_features=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Emil\\DAT550\\Final_Project\\1d.ipynb Cell 34'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000031?line=23'>24</a>\u001b[0m tweet_bow_test_d[n] \u001b[39m=\u001b[39m tweet_vec_d[n]\u001b[39m.\u001b[39mtransform(df_worthy_test[\u001b[39m\"\u001b[39m\u001b[39mtweet_text\u001b[39m\u001b[39m\"\u001b[39m]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000031?line=25'>26</a>\u001b[0m \u001b[39m# balanced upsample\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000031?line=26'>27</a>\u001b[0m tweet_vec_u[n] \u001b[39m=\u001b[39m cv_dict[n]\u001b[39m.\u001b[39;49mfit(df_upsampled[\u001b[39m\"\u001b[39;49m\u001b[39mtweet_text\u001b[39;49m\u001b[39m\"\u001b[39;49m]) \u001b[39m# DTM (CV, but normalized for relative frequency)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000031?line=27'>28</a>\u001b[0m tweet_bow_train_u[n] \u001b[39m=\u001b[39m tweet_vec_u[n]\u001b[39m.\u001b[39mtransform(df_upsampled[\u001b[39m\"\u001b[39m\u001b[39mtweet_text\u001b[39m\u001b[39m\"\u001b[39m]) \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000031?line=28'>29</a>\u001b[0m tweet_bow_test_u[n] \u001b[39m=\u001b[39m tweet_vec_u[n]\u001b[39m.\u001b[39mtransform(df_worthy_test[\u001b[39m\"\u001b[39m\u001b[39mtweet_text\u001b[39m\u001b[39m\"\u001b[39m]) \n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1283\u001b[0m, in \u001b[0;36mCountVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1266'>1267</a>\u001b[0m \u001b[39m\"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1267'>1268</a>\u001b[0m \n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1268'>1269</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1279'>1280</a>\u001b[0m \u001b[39m    Fitted vectorizer.\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1280'>1281</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1281'>1282</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[1;32m-> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1282'>1283</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1283'>1284</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1321'>1322</a>\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1322'>1323</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1323'>1324</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1324'>1325</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1325'>1326</a>\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1326'>1327</a>\u001b[0m             )\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1327'>1328</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1329'>1330</a>\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1331'>1332</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1332'>1333</a>\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1198'>1199</a>\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1199'>1200</a>\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[1;32m-> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1200'>1201</a>\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1201'>1202</a>\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=1202'>1203</a>\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:115\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=112'>113</a>\u001b[0m     doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=113'>114</a>\u001b[0m \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=114'>115</a>\u001b[0m     doc \u001b[39m=\u001b[39m tokenizer(doc)\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=115'>116</a>\u001b[0m \u001b[39mif\u001b[39;00m ngrams \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/sklearn/feature_extraction/text.py?line=116'>117</a>\u001b[0m     \u001b[39mif\u001b[39;00m stop_words \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Users\\Emil\\DAT550\\Final_Project\\1d.ipynb Cell 27'\u001b[0m in \u001b[0;36mtokenize_lemmatizer\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000025?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtokenize_lemmatizer\u001b[39m(text):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000025?line=10'>11</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenize_text(text, method\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mlemmatizer\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\Emil\\DAT550\\Final_Project\\1d.ipynb Cell 26'\u001b[0m in \u001b[0;36mtokenize_text\u001b[1;34m(text, method)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000024?line=28'>29</a>\u001b[0m     tokens \u001b[39m=\u001b[39m [x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tokens \u001b[39mif\u001b[39;00m remove_emoji(x) \u001b[39m==\u001b[39m x]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000024?line=29'>30</a>\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(tokens)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000024?line=30'>31</a>\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000024?line=31'>32</a>\u001b[0m     tokens \u001b[39m=\u001b[39m [token\u001b[39m.\u001b[39mlemma_ \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m doc]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Emil/DAT550/Final_Project/1d.ipynb#ch0000024?line=33'>34</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokens\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\language.py:1017\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1014'>1015</a>\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1015'>1016</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1016'>1017</a>\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1017'>1018</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1018'>1019</a>\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/language.py?line=1019'>1020</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:250\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.predict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\pipeline\\transition_parser.pyx:265\u001b[0m, in \u001b[0;36mspacy.pipeline.transition_parser.Parser.greedy_parse\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=310'>311</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=311'>312</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=312'>313</a>\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=313'>314</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=314'>315</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\ml\\tb_framework.py:33\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=31'>32</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model, X, is_train):\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=32'>33</a>\u001b[0m     step_model \u001b[39m=\u001b[39m ParserStepModel(\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=33'>34</a>\u001b[0m         X,\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=34'>35</a>\u001b[0m         model\u001b[39m.\u001b[39;49mlayers,\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=35'>36</a>\u001b[0m         unseen_classes\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39munseen_classes\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=36'>37</a>\u001b[0m         train\u001b[39m=\u001b[39;49mis_train,\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=37'>38</a>\u001b[0m         has_upper\u001b[39m=\u001b[39;49mmodel\u001b[39m.\u001b[39;49mattrs[\u001b[39m\"\u001b[39;49m\u001b[39mhas_upper\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=38'>39</a>\u001b[0m     )\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/spacy/ml/tb_framework.py?line=40'>41</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m step_model, step_model\u001b[39m.\u001b[39mfinish_steps\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\spacy\\ml\\parser_model.pyx:216\u001b[0m, in \u001b[0;36mspacy.ml.parser_model.ParserStepModel.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=51'>52</a>\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=53'>54</a>\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=54'>55</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=55'>56</a>\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=51'>52</a>\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=53'>54</a>\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=54'>55</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=55'>56</a>\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "    \u001b[1;31m[... skipping similar frames: Model.__call__ at line 291 (1 times)]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=51'>52</a>\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=53'>54</a>\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=54'>55</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=55'>56</a>\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\with_array.py:30\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=27'>28</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[39mbool\u001b[39m):\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=28'>29</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Ragged):\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=29'>30</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _ragged_forward(\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=30'>31</a>\u001b[0m             cast(Model[Ragged, Ragged], model), cast(Ragged, Xseq), is_train\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=31'>32</a>\u001b[0m         )\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=32'>33</a>\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Padded):\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=33'>34</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m _padded_forward(\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=34'>35</a>\u001b[0m             cast(Model[Padded, Padded], model), cast(Padded, Xseq), is_train\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=35'>36</a>\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\with_array.py:90\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=85'>86</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ragged_forward\u001b[39m(\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=86'>87</a>\u001b[0m     model: Model[Ragged, Ragged], Xr: Ragged, is_train: \u001b[39mbool\u001b[39m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=87'>88</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=88'>89</a>\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=89'>90</a>\u001b[0m     Y, get_dX \u001b[39m=\u001b[39m layer(Xr\u001b[39m.\u001b[39;49mdataXd, is_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=91'>92</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYr: Ragged) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Ragged:\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/with_array.py?line=92'>93</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[39m.\u001b[39mdataXd), dYr\u001b[39m.\u001b[39mlengths)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=51'>52</a>\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=53'>54</a>\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=54'>55</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=55'>56</a>\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\chain.py:54\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=51'>52</a>\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=52'>53</a>\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=53'>54</a>\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=54'>55</a>\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/chain.py?line=55'>56</a>\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=287'>288</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=288'>289</a>\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=289'>290</a>\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/model.py?line=290'>291</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32mc:\\Users\\Emil\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\thinc\\layers\\maxout.py:49\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/maxout.py?line=46'>47</a>\u001b[0m W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_param(\u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/maxout.py?line=47'>48</a>\u001b[0m W \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape2f(W, nO \u001b[39m*\u001b[39m nP, nI)\n\u001b[1;32m---> <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/maxout.py?line=48'>49</a>\u001b[0m Y \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mgemm(X, W, trans2\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/maxout.py?line=49'>50</a>\u001b[0m Y \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape1f(b, nO \u001b[39m*\u001b[39m nP)\n\u001b[0;32m     <a href='file:///c%3A/Users/Emil/AppData/Local/Programs/Python/Python310/lib/site-packages/thinc/layers/maxout.py?line=50'>51</a>\u001b[0m Z \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mreshape3f(Y, Y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], nO, nP)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tweet_vec = {}\n",
    "tweet_bow_train = {}\n",
    "tweet_bow_test = {}\n",
    "\n",
    "tweet_vec_d = {}\n",
    "tweet_bow_train_d = {}\n",
    "tweet_bow_test_d = {}\n",
    "\n",
    "tweet_vec_u = {}\n",
    "tweet_bow_train_u = {}\n",
    "tweet_bow_test_u = {}\n",
    "\n",
    "tweet_vec_h = {}\n",
    "tweet_bow_train_h = {}\n",
    "tweet_bow_test_h = {}\n",
    "for n in ngram_list:\n",
    "    tweet_vec[n] = cv_dict[n].fit(df_worthy_train[\"tweet_text\"]) # DTM (CV, but normalized for relative frequency)\n",
    "    tweet_bow_train[n] = tweet_vec[n].transform(df_worthy_train[\"tweet_text\"]) \n",
    "    tweet_bow_test[n] = tweet_vec[n].transform(df_worthy_test[\"tweet_text\"]) \n",
    "    \n",
    "    # balanced downsample\n",
    "    tweet_vec_d[n] = cv_dict[n].fit(df_downsampled[\"tweet_text\"]) # DTM (CV, but normalized for relative frequency)\n",
    "    tweet_bow_train_d[n] = tweet_vec_d[n].transform(df_downsampled[\"tweet_text\"]) \n",
    "    tweet_bow_test_d[n] = tweet_vec_d[n].transform(df_worthy_test[\"tweet_text\"]) \n",
    "    \n",
    "    # balanced upsample\n",
    "    tweet_vec_u[n] = cv_dict[n].fit(df_upsampled[\"tweet_text\"]) # DTM (CV, but normalized for relative frequency)\n",
    "    tweet_bow_train_u[n] = tweet_vec_u[n].transform(df_upsampled[\"tweet_text\"]) \n",
    "    tweet_bow_test_u[n] = tweet_vec_u[n].transform(df_worthy_test[\"tweet_text\"]) \n",
    "    \n",
    "    # balanced hybridsample\n",
    "    tweet_vec_h[n] = cv_dict[n].fit(df_meansampled[\"tweet_text\"]) # DTM (CV, but normalized for relative frequency)\n",
    "    tweet_bow_train_h[n] = tweet_vec_h[n].transform(df_meansampled[\"tweet_text\"]) \n",
    "    tweet_bow_test_h[n] = tweet_vec_h[n].transform(df_worthy_test[\"tweet_text\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dummy prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Unbalanced--------\n",
      "1-gram Priors: [0.05209274 0.85847636 0.00150557 0.04155375 0.01445348 0.00361337\n",
      " 0.00813008 0.01264679 0.00752785]\n",
      "Dummy F1-Score for 1-gram: 91.98%\n",
      "Dummy Precision-Score for 1-gram: 11.11%\n",
      "Dummy Precision-Score for 1-gram: 9.46%\n",
      "--------Downsampled--------\n",
      "1-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 1-gram: 11.41%\n",
      "Dummy Precision-Score for 1-gram: 11.11%\n",
      "Dummy Precision-Score for 1-gram: 0.67%\n",
      "--------Upsampled--------\n",
      "1-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 1-gram: 11.41%\n",
      "Dummy Precision-Score for 1-gram: 11.11%\n",
      "Dummy Precision-Score for 1-gram: 0.67%\n",
      "--------Meansampled--------\n",
      "1-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 1-gram: 11.41%\n",
      "Dummy Precision-Score for 1-gram: 11.11%\n",
      "Dummy Precision-Score for 1-gram: 0.67%\n",
      "--------------------------------------\n",
      "--------Unbalanced--------\n",
      "2-gram Priors: [0.05209274 0.85847636 0.00150557 0.04155375 0.01445348 0.00361337\n",
      " 0.00813008 0.01264679 0.00752785]\n",
      "Dummy F1-Score for 2-gram: 91.98%\n",
      "Dummy Precision-Score for 2-gram: 11.11%\n",
      "Dummy Precision-Score for 2-gram: 9.46%\n",
      "--------Downsampled--------\n",
      "2-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 2-gram: 11.41%\n",
      "Dummy Precision-Score for 2-gram: 11.11%\n",
      "Dummy Precision-Score for 2-gram: 0.67%\n",
      "--------Upsampled--------\n",
      "2-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 2-gram: 11.41%\n",
      "Dummy Precision-Score for 2-gram: 11.11%\n",
      "Dummy Precision-Score for 2-gram: 0.67%\n",
      "--------Meansampled--------\n",
      "2-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 2-gram: 11.41%\n",
      "Dummy Precision-Score for 2-gram: 11.11%\n",
      "Dummy Precision-Score for 2-gram: 0.67%\n",
      "--------------------------------------\n",
      "--------Unbalanced--------\n",
      "3-gram Priors: [0.05209274 0.85847636 0.00150557 0.04155375 0.01445348 0.00361337\n",
      " 0.00813008 0.01264679 0.00752785]\n",
      "Dummy F1-Score for 3-gram: 91.98%\n",
      "Dummy Precision-Score for 3-gram: 11.11%\n",
      "Dummy Precision-Score for 3-gram: 9.46%\n",
      "--------Downsampled--------\n",
      "3-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 3-gram: 11.41%\n",
      "Dummy Precision-Score for 3-gram: 11.11%\n",
      "Dummy Precision-Score for 3-gram: 0.67%\n",
      "--------Upsampled--------\n",
      "3-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 3-gram: 11.41%\n",
      "Dummy Precision-Score for 3-gram: 11.11%\n",
      "Dummy Precision-Score for 3-gram: 0.67%\n",
      "--------Meansampled--------\n",
      "3-gram Priors: [0.11111111 0.11111111 0.11111111 0.11111111 0.11111111 0.11111111\n",
      " 0.11111111 0.11111111 0.11111111]\n",
      "Dummy F1-Score for 3-gram: 11.41%\n",
      "Dummy Precision-Score for 3-gram: 11.11%\n",
      "Dummy Precision-Score for 3-gram: 0.67%\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import metrics\n",
    "\n",
    "for n in ngram_list:\n",
    "    print(\"--------Unbalanced--------\")\n",
    "    dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "    dummy_clf.fit(tweet_bow_train[n], df_worthy_train[\"class_label\"])\n",
    "    print(\"{}-gram Priors: {}\".format(n, dummy_clf.class_prior_))\n",
    "\n",
    "    f1_score = metrics.f1_score(dummy_clf.predict(tweet_bow_test[n]), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Dummy F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(dummy_clf.predict(tweet_bow_test[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(dummy_clf.predict(tweet_bow_test[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------Downsampled--------\")\n",
    "    dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "    dummy_clf.fit(tweet_bow_train_d[n], df_downsampled[\"class_label\"])\n",
    "    print(\"{}-gram Priors: {}\".format(n, dummy_clf.class_prior_))\n",
    "\n",
    "    f1_score = metrics.f1_score(dummy_clf.predict(tweet_bow_test_d[n]), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Dummy F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(dummy_clf.predict(tweet_bow_test_d[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(dummy_clf.predict(tweet_bow_test_d[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------Upsampled--------\")\n",
    "    dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "    dummy_clf.fit(tweet_bow_train_u[n], df_upsampled[\"class_label\"])\n",
    "    print(\"{}-gram Priors: {}\".format(n, dummy_clf.class_prior_))\n",
    "\n",
    "    f1_score = metrics.f1_score(dummy_clf.predict(tweet_bow_test_u[n]), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Dummy F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(dummy_clf.predict(tweet_bow_test_u[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(dummy_clf.predict(tweet_bow_test_u[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------Meansampled--------\")\n",
    "    dummy_clf = DummyClassifier(strategy=\"prior\")\n",
    "    dummy_clf.fit(tweet_bow_train_h[n], df_meansampled[\"class_label\"])\n",
    "    print(\"{}-gram Priors: {}\".format(n, dummy_clf.class_prior_))\n",
    "\n",
    "    f1_score = metrics.f1_score(dummy_clf.predict(tweet_bow_test_h[n]), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Dummy F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(dummy_clf.predict(tweet_bow_test_h[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(dummy_clf.predict(tweet_bow_test_h[n]), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Dummy Precision-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_w = LogisticRegression(class_weight=weights)\n",
    "lr_n = LogisticRegression(class_weight=None)\n",
    "\n",
    "lr_d = LogisticRegression(class_weight=None)\n",
    "lr_u = LogisticRegression(class_weight=None)\n",
    "lr_h = LogisticRegression(class_weight=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Custom weight--------\n",
      "Logistic Regression F1-Score for 1-gram: 80.03%\n",
      "Logistic Regression Precision-Score for 1-gram: 26.5%\n",
      "Logistic Regression Recall-Score for 1-gram: 23.42%\n",
      "--------No weight--------\n",
      "Logistic Regression F1-Score for 1-gram: 88.1%\n",
      "Logistic Regression Precision-Score for 1-gram: 19.58%\n",
      "Logistic Regression Recall-Score for 1-gram: 34.89%\n",
      "--------------------------------------\n",
      "--------Custom weight--------\n",
      "Logistic Regression F1-Score for 2-gram: 79.3%\n",
      "Logistic Regression Precision-Score for 2-gram: 24.46%\n",
      "Logistic Regression Recall-Score for 2-gram: 20.57%\n",
      "--------No weight--------\n",
      "Logistic Regression F1-Score for 2-gram: 90.88%\n",
      "Logistic Regression Precision-Score for 2-gram: 14.15%\n",
      "Logistic Regression Recall-Score for 2-gram: 26.52%\n",
      "--------------------------------------\n",
      "--------Custom weight--------\n",
      "Logistic Regression F1-Score for 3-gram: 80.43%\n",
      "Logistic Regression Precision-Score for 3-gram: 16.26%\n",
      "Logistic Regression Recall-Score for 3-gram: 19.5%\n",
      "--------No weight--------\n",
      "Logistic Regression F1-Score for 3-gram: 91.85%\n",
      "Logistic Regression Precision-Score for 3-gram: 11.92%\n",
      "Logistic Regression Recall-Score for 3-gram: 20.61%\n",
      "--------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for n in ngram_list:\n",
    "    lr_w.fit(tweet_bow_train[n].toarray(), df_worthy_train[\"class_label\"])\n",
    "    lr_n.fit(tweet_bow_train[n].toarray(), df_worthy_train[\"class_label\"])\n",
    "    \n",
    "    print(\"--------Custom weight--------\")\n",
    "    f1_score = metrics.f1_score(lr_w.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Logistic Regression F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(lr_w.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(lr_w.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------No weight--------\")\n",
    "    f1_score = metrics.f1_score(lr_n.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Logistic Regression F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(lr_n.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(lr_n.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Downsampled--------\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [909, 45]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17960/1586547565.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"--------Downsampled--------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mf1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_d\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet_bow_test_d\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf_downsampled\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"class_label\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"weighted\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Logistic Regression F1-Score for {}-gram: {}%\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.66666667\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m        \u001b[1;33m,\u001b[0m \u001b[1;36m0.66666667\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m     \"\"\"\n\u001b[1;32m-> 1132\u001b[1;33m     return fbeta_score(\n\u001b[0m\u001b[0;32m   1133\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1268\u001b[0m     \"\"\"\n\u001b[0;32m   1269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1270\u001b[1;33m     _, _, f, _ = precision_recall_fscore_support(\n\u001b[0m\u001b[0;32m   1271\u001b[0m         \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[0;32m   1554\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1556\u001b[1;33m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_set_wise_labels\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos_label\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1558\u001b[0m     \u001b[1;31m# Calculate tp_sum, pred_sum, true_sum ###\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[1;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"average has to be one of \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1357\u001b[1;33m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1358\u001b[0m     \u001b[1;31m# Convert to Python primitive type to avoid NumPy type / Python str\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m     \u001b[1;31m# comparison. See https://github.com/numpy/numpy/issues/6784\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\metrics\\_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0marray\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \"\"\"\n\u001b[1;32m---> 84\u001b[1;33m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_true\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"y_pred\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\minh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    381\u001b[0m     \u001b[0muniques\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 383\u001b[1;33m         raise ValueError(\n\u001b[0m\u001b[0;32m    384\u001b[0m             \u001b[1;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m             \u001b[1;33m%\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [909, 45]"
     ]
    }
   ],
   "source": [
    "for n in ngram_list:\n",
    "    lr_d.fit(tweet_bow_train_d[n].toarray(), df_downsampled[\"class_label\"])\n",
    "    lr_u.fit(tweet_bow_train_u[n].toarray(), df_upsampled[\"class_label\"])\n",
    "    lr_h.fit(tweet_bow_train_h[n].toarray(), df_meansampled[\"class_label\"])\n",
    "    \n",
    "    # print(\"--------Downsampled--------\")\n",
    "    # f1_score = metrics.f1_score(lr_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"Logistic Regression F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(lr_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"Logistic Regression Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(lr_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"Logistic Regression Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------Upsampled--------\")\n",
    "    f1_score = metrics.f1_score(lr_u.predict(tweet_bow_test_u[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Logistic Regression F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(lr_u.predict(tweet_bow_test_u[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(lr_u.predict(tweet_bow_test_u[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    print(\"--------Meansampled--------\")\n",
    "    f1_score = metrics.f1_score(lr_h.predict(tweet_bow_test_h[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Logistic Regression F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(lr_h.predict(tweet_bow_test_h[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(lr_h.predict(tweet_bow_test_h[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Logistic Regression Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-nearest neighbor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://iq.opengenus.org/text-classification-using-k-nearest-neighbors/\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9)#, weights=class_weights)\n",
    "knn_d = KNeighborsClassifier(n_neighbors=9)#, weights=class_weights)\n",
    "knn_u = KNeighborsClassifier(n_neighbors=9)#, weights=class_weights)\n",
    "knn_h = KNeighborsClassifier(n_neighbors=9)#, weights=class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Unbalanced--------\n",
      "KNN F1-Score for 1-gram: 91.98%\n",
      "KNN Precision-Score for 1-gram: 11.11%\n",
      "KNN Recall-Score for 1-gram: 9.46%\n",
      "--------Unbalanced--------\n",
      "KNN F1-Score for 2-gram: 91.98%\n",
      "KNN Precision-Score for 2-gram: 11.11%\n",
      "KNN Recall-Score for 2-gram: 9.46%\n",
      "--------Unbalanced--------\n",
      "KNN F1-Score for 3-gram: 91.65%\n",
      "KNN Precision-Score for 3-gram: 11.08%\n",
      "KNN Recall-Score for 3-gram: 9.46%\n"
     ]
    }
   ],
   "source": [
    "for n in ngram_list:\n",
    "    knn.fit(tweet_bow_train[n].toarray(), df_worthy_train[\"class_label\"])\n",
    "    knn_d.fit(tweet_bow_train_d[n].toarray(), df_downsampled[\"class_label\"])\n",
    "    knn_u.fit(tweet_bow_train_u[n].toarray(), df_upsampled[\"class_label\"])\n",
    "    knn_h.fit(tweet_bow_train_h[n].toarray(), df_meansampled[\"class_label\"])\n",
    "    \n",
    "    print(\"--------Unbalanced--------\")\n",
    "    f1_score = metrics.f1_score(knn.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(knn.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(knn.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Downsampled--------\")\n",
    "    # f1_score = metrics.f1_score(knn_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(knn_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(knn_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Upsampled--------\")\n",
    "    # f1_score = metrics.f1_score(knn_u.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(knn_u.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(knn_u.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Meansampled--------\")\n",
    "    # f1_score = metrics.f1_score(knn_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(knn_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(knn_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    # print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Unbalanced--------\n",
      "Complement F1-Score for 1-gram: 90.04%\n",
      "Complement precision for 1-gram: 13.09%\n",
      "Complement recall for 1-gram: 18.76%\n",
      "--------Unbalanced--------\n",
      "Complement F1-Score for 2-gram: 90.74%\n",
      "Complement precision for 2-gram: 14.23%\n",
      "Complement recall for 2-gram: 26.29%\n",
      "--------Unbalanced--------\n",
      "Complement F1-Score for 3-gram: 91.5%\n",
      "Complement precision for 3-gram: 12.2%\n",
      "Complement recall for 3-gram: 23.95%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # suited for classification with discrete features\n",
    "from sklearn.naive_bayes import CategoricalNB \n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf_d = MultinomialNB()\n",
    "clf_u = MultinomialNB()\n",
    "clf_h = MultinomialNB()\n",
    "\n",
    "for n in ngram_list:\n",
    "    clf.fit(tweet_bow_train[n].toarray(), df_worthy_train[\"class_label\"])\n",
    "    clf_d.fit(tweet_bow_train_d[n].toarray(), df_downsampled[\"class_label\"])\n",
    "    clf_u.fit(tweet_bow_train_u[n].toarray(), df_upsampled[\"class_label\"])\n",
    "    clf_h.fit(tweet_bow_train_h[n].toarray(), df_meansampled[\"class_label\"])\n",
    "    \n",
    "    print(\"--------Unbalanced--------\")\n",
    "    f1_score = metrics.f1_score(clf.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"weighted\")\n",
    "    print(\"Complement F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    precision_score = metrics.precision_score(clf.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Complement precision for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    recall_score = metrics.recall_score(clf.predict(tweet_bow_test[n].toarray()), df_worthy_test[\"class_label\"], average=\"macro\")\n",
    "    print(\"Complement recall for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Downsampled--------\")\n",
    "    # f1_score = metrics.f1_score(clf_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(clf_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(clf_d.predict(tweet_bow_test_d[n].toarray()), df_downsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Upsampled--------\")\n",
    "    # f1_score = metrics.f1_score(clf_u.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"Complement F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(clf_u.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"Complement precision for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(clf.predict(tweet_bow_test_u[n].toarray()), df_upsampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"Complement recall for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    \n",
    "    # print(\"--------Meansampled--------\")\n",
    "    # f1_score = metrics.f1_score(clf_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"weighted\")\n",
    "    # print(\"KNN F1-Score for {}-gram: {}%\".format(n, round(f1_score * 100, 2)))\n",
    "    \n",
    "    # precision_score = metrics.precision_score(clf_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Precision-Score for {}-gram: {}%\".format(n, round(precision_score * 100, 2)))\n",
    "    \n",
    "    # recall_score = metrics.recall_score(clf_h.predict(tweet_bow_test_h[n].toarray()), df_meansampled[\"class_label\"], average=\"macro\")\n",
    "    # print(\"KNN Recall-Score for {}-gram: {}%\".format(n, round(recall_score * 100, 2)))\n",
    "    # print(\"--------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br>\n",
    "\n",
    "<hr>\n",
    "\n",
    "\n",
    "# (Multi-Class) Text Classification using Bidirectional LSTM\n",
    "\n",
    "Source: https://medium.com/analytics-vidhya/author-multi-class-text-classification-using-bidirectional-lstm-keras-c9a533a1cc4a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLemmText(text):\n",
    " tokens=word_tokenize(text)\n",
    " lemmatizer = WordNetLemmatizer()\n",
    " tokens=[lemmatizer.lemmatize(word) for word in tokens]\n",
    " return \" \".join(tokens)\n",
    "df_worthy_train[\"tweet_text\"] = list(map(getLemmText, df_worthy_train[\"tweet_text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIMENSION = 64\n",
    "VOCABULARY_SIZE = 2000\n",
    "MAX_LENGTH = 100\n",
    "OOV_TOK = '<OOV>'\n",
    "TRUNCATE_TYPE = 'post'\n",
    "PADDING_TYPE = 'post'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=VOCABULARY_SIZE, oov_token=OOV_TOK)\n",
    "tokenizer.fit_on_texts(list(df_worthy_train[\"tweet_text\"]) + list(df_worthy_train[\"class_label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 15745\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<OOV>': 1,\n",
       " 'the': 2,\n",
       " 'and': 3,\n",
       " 'vaccine': 4,\n",
       " 't': 5,\n",
       " 'co': 6,\n",
       " 'to': 7,\n",
       " 'http': 8,\n",
       " 'not': 9,\n",
       " 'no': 10}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain_sequences = tokenizer.texts_to_sequences(df_worthy_train[\"tweet_text\"])\n",
    "xtest_sequences = tokenizer.texts_to_sequences(df_worthy_train[\"class_label\"])\n",
    "word_index = tokenizer.word_index\n",
    "print('Vocabulary size:', len(word_index))\n",
    "dict(list(word_index.items())[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40, 50, 369, 24, 5, 1084, 70, 4, 122, 3, 70, 351, 257, 122, 26, 2, 158, 7, 90, 36, 2, 521, 72, 317, 17, 12, 14, 1]\n"
     ]
    }
   ],
   "source": [
    "print(xtrain_sequences[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "100\n",
      "[  40   50  369   24    5 1084   70    4  122    3   70  351  257  122\n",
      "   26    2  158    7   90   36    2  521   72  317   17   12   14    1\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "xtrain_pad = sequence.pad_sequences(xtrain_sequences, maxlen=MAX_LENGTH, padding=PADDING_TYPE, truncating=TRUNCATE_TYPE)\n",
    "xtest_pad = sequence.pad_sequences(xtest_sequences, maxlen=MAX_LENGTH, padding=PADDING_TYPE, truncating=TRUNCATE_TYPE)\n",
    "print(len(xtrain_sequences[0]))\n",
    "print(len(xtrain_pad[0]))\n",
    "print(xtrain_pad[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1051, 607, 147, 19, 70, 1052, 415, 30, 8, 10, 42, 200, 6, 1, 416, 29, 1053, 2, 1054, 608, 18, 1055, 417, 1056, 2, 1057, 416, 251, 20, 64, 418, 96, 7, 19, 419, 70, 113, 18, 20, 17, 1, 201]\n",
      "[1058, 609, 420, 14, 56, 307, 8, 10, 7, 11, 1059, 6, 202, 308, 309, 1060, 2, 1061, 89, 2, 119, 5, 3, 4, 1062]\n",
      "[8, 10, 25, 1063, 22, 201, 9, 252, 2, 18, 8, 10, 7, 77, 6, 421, 2, 159, 6, 160, 23, 14, 22, 148, 6, 422, 65, 22, 1064, 2, 97, 22, 161, 5, 3, 4, 1065]\n",
      "(306,)\n"
     ]
    }
   ],
   "source": [
    "label_tokenizer = Tokenizer()\n",
    "label_tokenizer.fit_on_texts(list(df_worthy_valid[\"tweet_text\"]))\n",
    "training_label_seq = np.array(label_tokenizer.texts_to_sequences(df_worthy_valid[\"tweet_text\"]))\n",
    "test_label_seq = np.array(label_tokenizer.texts_to_sequences(df_worthy_valid[\"class_label\"]))\n",
    "print(training_label_seq[0])\n",
    "print(training_label_seq[1])\n",
    "print(training_label_seq[2])\n",
    "print(training_label_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dear <OOV> for the love of god and family do not let anyone jab your <OOV> with the experimental covid 19 vaccine http t co <OOV> ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ? ?\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "def decode_article(text):\n",
    " return \" \".join([reverse_word_index.get(i, \"?\") for i in text])\n",
    "print(decode_article(xtrain_pad[11]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1,\n",
    "                     EMBEDDING_DIMENSION))\n",
    "model.add(SpatialDropout1D(0.3))\n",
    "model.add(Bidirectional(LSTM(EMBEDDING_DIMENSION, dropout=0.3, recurrent_dropout=0.3)))\n",
    "model.add(Dense(EMBEDDING_DIMENSION, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(EMBEDDING_DIMENSION, activation='relu'))\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(306,)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: below codeblock doesnt work due to these two fucks having the wrong datatype\n",
    "xtrain_pad = np.asarray(xtrain_pad).astype(np.int_)\n",
    "#training_label_seq\n",
    "\n",
    "#these too?\n",
    "#xtest_pad\n",
    "#test_label_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type list).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/ivarw/Dropbox/GitDAT550/Final_Project/1d.ipynb Cell 58'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivarw/Dropbox/GitDAT550/Final_Project/1d.ipynb#ch0000061?line=0'>1</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ivarw/Dropbox/GitDAT550/Final_Project/1d.ipynb#ch0000061?line=1'>2</a>\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(xtrain_pad, training_label_seq, epochs\u001b[39m=\u001b[39;49mnum_epochs, validation_data\u001b[39m=\u001b[39;49m(xtest_pad, test_label_seq), verbose\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/miniforge3/envs/DAT540/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     <a href='file:///Users/ivarw/miniforge3/envs/DAT540/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=64'>65</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     <a href='file:///Users/ivarw/miniforge3/envs/DAT540/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=65'>66</a>\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> <a href='file:///Users/ivarw/miniforge3/envs/DAT540/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=66'>67</a>\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m     <a href='file:///Users/ivarw/miniforge3/envs/DAT540/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=67'>68</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     <a href='file:///Users/ivarw/miniforge3/envs/DAT540/lib/python3.8/site-packages/keras/utils/traceback_utils.py?line=68'>69</a>\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniforge3/envs/DAT540/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/ivarw/miniforge3/envs/DAT540/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=99'>100</a>\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[1;32m    <a href='file:///Users/ivarw/miniforge3/envs/DAT540/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=100'>101</a>\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m--> <a href='file:///Users/ivarw/miniforge3/envs/DAT540/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py?line=101'>102</a>\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type list)."
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "history = model.fit(xtrain_pad, training_label_seq, epochs=num_epochs, validation_data=(xtest_pad, test_label_seq), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ivarw/Dropbox/GitDAT550/Final_Project/1d.ipynb Cell 60'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivarw/Dropbox/GitDAT550/Final_Project/1d.ipynb#ch0000062?line=5'>6</a>\u001b[0m   plt\u001b[39m.\u001b[39mlegend([string, \u001b[39m'\u001b[39m\u001b[39mval_\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mstring])\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ivarw/Dropbox/GitDAT550/Final_Project/1d.ipynb#ch0000062?line=6'>7</a>\u001b[0m   plt\u001b[39m.\u001b[39mshow()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ivarw/Dropbox/GitDAT550/Final_Project/1d.ipynb#ch0000062?line=8'>9</a>\u001b[0m graph_plots(history, \u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ivarw/Dropbox/GitDAT550/Final_Project/1d.ipynb#ch0000062?line=9'>10</a>\u001b[0m graph_plots(history, \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "def graph_plots(history, string):\n",
    "  plt.plot(history.history[string])\n",
    "  plt.plot(history.history['val_'+string])\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(string)\n",
    "  plt.legend([string, 'val_'+string])\n",
    "  plt.show()\n",
    "  \n",
    "graph_plots(history, \"accuracy\")\n",
    "graph_plots(history, \"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e35155524c1f9408bca4749410ef0ff0fb980c2b8ff5cdb8c228ddba1c490ab"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('DAT540')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
